{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fercho1011/Statistical-LearningII-ProyectoFinal/blob/master/ANN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "332Z8kH6U3ZO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "329b1a67-f221-420a-e4ff-8de2e6c8ea74"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras import regularizers\n",
        "from keras import optimizers\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-4V7mX2U55Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8iLpyuAX9dl",
        "colab_type": "code",
        "outputId": "c641ab52-fa1c-4de2-c78a-ac780c6a35e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "data = pd.read_csv(r'/content/drive/My Drive/Colab Notebooks/kc_house_data.csv')\n",
        "data.columns\n",
        "\n",
        "clean_data= data.dropna(subset=['price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', \n",
        "                                'floors', 'waterfront', 'view', 'condition', 'grade',\n",
        "                                'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode',\n",
        "                                'lat', 'long', 'sqft_living15', 'sqft_lot15'])\n",
        "#No NAs\n",
        "len(data)==len(clean_data)\n",
        "\n",
        "#Creating encoding for categorical variables\n",
        "data[0:5]\n",
        "cat_feats = ['zipcode']\n",
        "for feat in cat_feats:\n",
        "    number = LabelEncoder()\n",
        "    data[feat] = number.fit_transform(data[feat].astype('str'))\n",
        "\n",
        "non_norm_cols= ['price', 'id', 'zipcode', 'date']\n",
        "feats = list(set(list(data.columns))-set(non_norm_cols))\n",
        "\n",
        "data[feats]= data[feats]/data[feats].max()\n",
        "data\n",
        "\n",
        "price = data['price']\n",
        "factors = data.drop(columns=['price'])\n",
        "factors = factors.drop(columns=['date', 'id'])\n",
        "\n",
        "Xtr, Xts, Ytr, Yts = train_test_split(factors,price, test_size=0.40, random_state=15)\n",
        "Xva, Xts, Yva, Yts = train_test_split(Xts,Yts, test_size=0.5, random_state=15)\n",
        "\n",
        "(len(Yva)+len(Ytr) + len(Yts))==len(data)\n",
        "#True, no losses\n",
        "\n",
        "Xtr"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bedrooms</th>\n",
              "      <th>bathrooms</th>\n",
              "      <th>sqft_living</th>\n",
              "      <th>sqft_lot</th>\n",
              "      <th>floors</th>\n",
              "      <th>waterfront</th>\n",
              "      <th>view</th>\n",
              "      <th>condition</th>\n",
              "      <th>grade</th>\n",
              "      <th>sqft_above</th>\n",
              "      <th>sqft_basement</th>\n",
              "      <th>yr_built</th>\n",
              "      <th>yr_renovated</th>\n",
              "      <th>zipcode</th>\n",
              "      <th>lat</th>\n",
              "      <th>long</th>\n",
              "      <th>sqft_living15</th>\n",
              "      <th>sqft_lot15</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10952</th>\n",
              "      <td>0.060606</td>\n",
              "      <td>0.21875</td>\n",
              "      <td>0.158050</td>\n",
              "      <td>0.003028</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>0.106270</td>\n",
              "      <td>0.236515</td>\n",
              "      <td>0.957816</td>\n",
              "      <td>0.988089</td>\n",
              "      <td>48</td>\n",
              "      <td>0.996877</td>\n",
              "      <td>1.008045</td>\n",
              "      <td>0.362319</td>\n",
              "      <td>0.004591</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1699</th>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.25000</td>\n",
              "      <td>0.147710</td>\n",
              "      <td>0.004239</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>0.212540</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.950868</td>\n",
              "      <td>0.985608</td>\n",
              "      <td>58</td>\n",
              "      <td>0.995136</td>\n",
              "      <td>1.008886</td>\n",
              "      <td>0.296296</td>\n",
              "      <td>0.008035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1721</th>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.31250</td>\n",
              "      <td>0.120384</td>\n",
              "      <td>0.007020</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>0.173220</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.986104</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>9</td>\n",
              "      <td>0.999577</td>\n",
              "      <td>1.007526</td>\n",
              "      <td>0.273752</td>\n",
              "      <td>0.010158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15455</th>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.21875</td>\n",
              "      <td>0.097489</td>\n",
              "      <td>0.003652</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>0.140276</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.977171</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>28</td>\n",
              "      <td>0.998361</td>\n",
              "      <td>1.006718</td>\n",
              "      <td>0.296296</td>\n",
              "      <td>0.007536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11423</th>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.28125</td>\n",
              "      <td>0.135155</td>\n",
              "      <td>0.004633</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>0.194474</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.985608</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>40</td>\n",
              "      <td>0.989254</td>\n",
              "      <td>1.007452</td>\n",
              "      <td>0.347826</td>\n",
              "      <td>0.009690</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>291</th>\n",
              "      <td>0.121212</td>\n",
              "      <td>0.21875</td>\n",
              "      <td>0.217873</td>\n",
              "      <td>0.005465</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>0.189160</td>\n",
              "      <td>0.242739</td>\n",
              "      <td>0.980149</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>22</td>\n",
              "      <td>0.998644</td>\n",
              "      <td>1.007485</td>\n",
              "      <td>0.341385</td>\n",
              "      <td>0.011019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18612</th>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.31250</td>\n",
              "      <td>0.124815</td>\n",
              "      <td>0.003107</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>0.179596</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.963275</td>\n",
              "      <td>0.991563</td>\n",
              "      <td>56</td>\n",
              "      <td>0.994680</td>\n",
              "      <td>1.008779</td>\n",
              "      <td>0.138486</td>\n",
              "      <td>0.005896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9146</th>\n",
              "      <td>0.151515</td>\n",
              "      <td>0.31250</td>\n",
              "      <td>0.180945</td>\n",
              "      <td>0.004235</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>0.260361</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.993548</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>18</td>\n",
              "      <td>0.991201</td>\n",
              "      <td>1.007229</td>\n",
              "      <td>0.312399</td>\n",
              "      <td>0.006927</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13155</th>\n",
              "      <td>0.151515</td>\n",
              "      <td>0.34375</td>\n",
              "      <td>0.200886</td>\n",
              "      <td>0.117119</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>0.180659</td>\n",
              "      <td>0.211618</td>\n",
              "      <td>0.976675</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>40</td>\n",
              "      <td>0.989665</td>\n",
              "      <td>1.006644</td>\n",
              "      <td>0.454106</td>\n",
              "      <td>0.285000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12071</th>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.28125</td>\n",
              "      <td>0.128508</td>\n",
              "      <td>0.005746</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>0.125399</td>\n",
              "      <td>0.116183</td>\n",
              "      <td>0.981141</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>31</td>\n",
              "      <td>0.994899</td>\n",
              "      <td>1.007081</td>\n",
              "      <td>0.302738</td>\n",
              "      <td>0.009889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6771</th>\n",
              "      <td>0.151515</td>\n",
              "      <td>0.34375</td>\n",
              "      <td>0.282939</td>\n",
              "      <td>0.008357</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.407120</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.972208</td>\n",
              "      <td>0.982630</td>\n",
              "      <td>25</td>\n",
              "      <td>0.996113</td>\n",
              "      <td>1.007715</td>\n",
              "      <td>0.619968</td>\n",
              "      <td>0.041969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14809</th>\n",
              "      <td>0.121212</td>\n",
              "      <td>0.31250</td>\n",
              "      <td>0.168390</td>\n",
              "      <td>0.008653</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>0.160468</td>\n",
              "      <td>0.159751</td>\n",
              "      <td>0.963772</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>55</td>\n",
              "      <td>0.998489</td>\n",
              "      <td>1.008095</td>\n",
              "      <td>0.344605</td>\n",
              "      <td>0.011352</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17883</th>\n",
              "      <td>0.060606</td>\n",
              "      <td>0.12500</td>\n",
              "      <td>0.059084</td>\n",
              "      <td>0.003863</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>0.085016</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.962779</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>49</td>\n",
              "      <td>0.998187</td>\n",
              "      <td>1.008194</td>\n",
              "      <td>0.148148</td>\n",
              "      <td>0.006818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13362</th>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.31250</td>\n",
              "      <td>0.204579</td>\n",
              "      <td>0.006524</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.294368</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.988586</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7</td>\n",
              "      <td>0.995768</td>\n",
              "      <td>1.006636</td>\n",
              "      <td>0.407407</td>\n",
              "      <td>0.011964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11245</th>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.21875</td>\n",
              "      <td>0.125554</td>\n",
              "      <td>0.004561</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>0.180659</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.986104</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>18</td>\n",
              "      <td>0.991155</td>\n",
              "      <td>1.007097</td>\n",
              "      <td>0.272142</td>\n",
              "      <td>0.008500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13415</th>\n",
              "      <td>0.121212</td>\n",
              "      <td>0.25000</td>\n",
              "      <td>0.094535</td>\n",
              "      <td>0.004578</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.461538</td>\n",
              "      <td>0.136026</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.972208</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>32</td>\n",
              "      <td>0.993143</td>\n",
              "      <td>1.007089</td>\n",
              "      <td>0.201288</td>\n",
              "      <td>0.008827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14041</th>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.31250</td>\n",
              "      <td>0.198671</td>\n",
              "      <td>0.010574</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.769231</td>\n",
              "      <td>0.285866</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.991067</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>37</td>\n",
              "      <td>0.997413</td>\n",
              "      <td>1.006372</td>\n",
              "      <td>0.581320</td>\n",
              "      <td>0.019384</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3548</th>\n",
              "      <td>0.121212</td>\n",
              "      <td>0.31250</td>\n",
              "      <td>0.233383</td>\n",
              "      <td>0.021331</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.239107</td>\n",
              "      <td>0.188797</td>\n",
              "      <td>0.988586</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5</td>\n",
              "      <td>0.995596</td>\n",
              "      <td>1.006561</td>\n",
              "      <td>0.557166</td>\n",
              "      <td>0.019769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11491</th>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.12500</td>\n",
              "      <td>0.089365</td>\n",
              "      <td>0.004624</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>0.128587</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.968734</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>52</td>\n",
              "      <td>0.994979</td>\n",
              "      <td>1.007839</td>\n",
              "      <td>0.246377</td>\n",
              "      <td>0.006772</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18039</th>\n",
              "      <td>0.121212</td>\n",
              "      <td>0.31250</td>\n",
              "      <td>0.155096</td>\n",
              "      <td>0.005148</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>0.223167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.989082</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>12</td>\n",
              "      <td>0.987808</td>\n",
              "      <td>1.005729</td>\n",
              "      <td>0.342995</td>\n",
              "      <td>0.009826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14047</th>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.18750</td>\n",
              "      <td>0.108567</td>\n",
              "      <td>0.011178</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.461538</td>\n",
              "      <td>0.156217</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.950868</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>13</td>\n",
              "      <td>0.990636</td>\n",
              "      <td>1.008614</td>\n",
              "      <td>0.281804</td>\n",
              "      <td>0.018450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7361</th>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.31250</td>\n",
              "      <td>0.183900</td>\n",
              "      <td>0.014467</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.264612</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.989082</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>29</td>\n",
              "      <td>0.997781</td>\n",
              "      <td>1.005886</td>\n",
              "      <td>0.466989</td>\n",
              "      <td>0.039836</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20928</th>\n",
              "      <td>0.121212</td>\n",
              "      <td>0.31250</td>\n",
              "      <td>0.196455</td>\n",
              "      <td>0.002472</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>0.282678</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.997519</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>26</td>\n",
              "      <td>0.990870</td>\n",
              "      <td>1.006100</td>\n",
              "      <td>0.384863</td>\n",
              "      <td>0.005597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18581</th>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.21875</td>\n",
              "      <td>0.105613</td>\n",
              "      <td>0.009810</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>0.151966</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.976179</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>15</td>\n",
              "      <td>0.994656</td>\n",
              "      <td>1.006001</td>\n",
              "      <td>0.272142</td>\n",
              "      <td>0.015065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>638</th>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.25000</td>\n",
              "      <td>0.151403</td>\n",
              "      <td>0.008056</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>0.217853</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.989082</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2</td>\n",
              "      <td>0.989336</td>\n",
              "      <td>1.008202</td>\n",
              "      <td>0.351047</td>\n",
              "      <td>0.013303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6745</th>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.28125</td>\n",
              "      <td>0.116691</td>\n",
              "      <td>0.004809</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>0.167906</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.985608</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>13</td>\n",
              "      <td>0.989675</td>\n",
              "      <td>1.008589</td>\n",
              "      <td>0.278583</td>\n",
              "      <td>0.009241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11292</th>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.25000</td>\n",
              "      <td>0.169867</td>\n",
              "      <td>0.003028</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>0.122210</td>\n",
              "      <td>0.238589</td>\n",
              "      <td>0.961787</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>69</td>\n",
              "      <td>0.997373</td>\n",
              "      <td>1.009051</td>\n",
              "      <td>0.370370</td>\n",
              "      <td>0.005739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8879</th>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.21875</td>\n",
              "      <td>0.140251</td>\n",
              "      <td>0.006858</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>0.201807</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.964268</td>\n",
              "      <td>0.995037</td>\n",
              "      <td>68</td>\n",
              "      <td>0.992070</td>\n",
              "      <td>1.008119</td>\n",
              "      <td>0.322061</td>\n",
              "      <td>0.012000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11627</th>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.18750</td>\n",
              "      <td>0.115953</td>\n",
              "      <td>0.007516</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>0.166844</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.969727</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>63</td>\n",
              "      <td>0.992892</td>\n",
              "      <td>1.008523</td>\n",
              "      <td>0.342995</td>\n",
              "      <td>0.014247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14698</th>\n",
              "      <td>0.060606</td>\n",
              "      <td>0.18750</td>\n",
              "      <td>0.095273</td>\n",
              "      <td>0.003113</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>0.111583</td>\n",
              "      <td>0.049793</td>\n",
              "      <td>0.966253</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>49</td>\n",
              "      <td>0.998120</td>\n",
              "      <td>1.008037</td>\n",
              "      <td>0.207729</td>\n",
              "      <td>0.006205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9551</th>\n",
              "      <td>0.121212</td>\n",
              "      <td>0.34375</td>\n",
              "      <td>0.164697</td>\n",
              "      <td>0.005838</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>0.140276</td>\n",
              "      <td>0.188797</td>\n",
              "      <td>0.991563</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>55</td>\n",
              "      <td>0.998939</td>\n",
              "      <td>1.008325</td>\n",
              "      <td>0.338164</td>\n",
              "      <td>0.011019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9373</th>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.21875</td>\n",
              "      <td>0.093796</td>\n",
              "      <td>0.002916</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.461538</td>\n",
              "      <td>0.104145</td>\n",
              "      <td>0.060166</td>\n",
              "      <td>0.953846</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>46</td>\n",
              "      <td>0.995226</td>\n",
              "      <td>1.008202</td>\n",
              "      <td>0.204509</td>\n",
              "      <td>0.007382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19388</th>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.18750</td>\n",
              "      <td>0.093796</td>\n",
              "      <td>0.003096</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>0.134963</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.967742</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>49</td>\n",
              "      <td>0.997873</td>\n",
              "      <td>1.008020</td>\n",
              "      <td>0.254428</td>\n",
              "      <td>0.005831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17101</th>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.12500</td>\n",
              "      <td>0.089365</td>\n",
              "      <td>0.005827</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>0.128587</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.970223</td>\n",
              "      <td>0.997022</td>\n",
              "      <td>5</td>\n",
              "      <td>0.995713</td>\n",
              "      <td>1.007122</td>\n",
              "      <td>0.254428</td>\n",
              "      <td>0.011150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4908</th>\n",
              "      <td>0.151515</td>\n",
              "      <td>0.43750</td>\n",
              "      <td>0.258493</td>\n",
              "      <td>0.006536</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.769231</td>\n",
              "      <td>0.371945</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.990571</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>38</td>\n",
              "      <td>0.996046</td>\n",
              "      <td>1.005432</td>\n",
              "      <td>0.500805</td>\n",
              "      <td>0.012439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.12500</td>\n",
              "      <td>0.092319</td>\n",
              "      <td>0.005919</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>0.132837</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.977171</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2</td>\n",
              "      <td>0.990722</td>\n",
              "      <td>1.008169</td>\n",
              "      <td>0.206119</td>\n",
              "      <td>0.010158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10021</th>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.31250</td>\n",
              "      <td>0.118168</td>\n",
              "      <td>0.000794</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>0.170032</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.995037</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>51</td>\n",
              "      <td>0.998173</td>\n",
              "      <td>1.008894</td>\n",
              "      <td>0.223833</td>\n",
              "      <td>0.001516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1063</th>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.12500</td>\n",
              "      <td>0.075332</td>\n",
              "      <td>0.010930</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>0.108395</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.977171</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>15</td>\n",
              "      <td>0.994721</td>\n",
              "      <td>1.006001</td>\n",
              "      <td>0.281804</td>\n",
              "      <td>0.013361</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13161</th>\n",
              "      <td>0.121212</td>\n",
              "      <td>0.31250</td>\n",
              "      <td>0.189069</td>\n",
              "      <td>0.007727</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>0.187035</td>\n",
              "      <td>0.165975</td>\n",
              "      <td>0.967246</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>49</td>\n",
              "      <td>0.998265</td>\n",
              "      <td>1.008144</td>\n",
              "      <td>0.283414</td>\n",
              "      <td>0.007328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8388</th>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.25000</td>\n",
              "      <td>0.127770</td>\n",
              "      <td>0.005468</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>0.183847</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.970720</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6</td>\n",
              "      <td>0.996021</td>\n",
              "      <td>1.006751</td>\n",
              "      <td>0.236715</td>\n",
              "      <td>0.010365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8335</th>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.31250</td>\n",
              "      <td>0.125554</td>\n",
              "      <td>0.003754</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>0.180659</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.991067</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>40</td>\n",
              "      <td>0.989210</td>\n",
              "      <td>1.007468</td>\n",
              "      <td>0.276973</td>\n",
              "      <td>0.007122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19210</th>\n",
              "      <td>0.151515</td>\n",
              "      <td>0.21875</td>\n",
              "      <td>0.143279</td>\n",
              "      <td>0.004029</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>0.206164</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.969231</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>63</td>\n",
              "      <td>0.993484</td>\n",
              "      <td>1.008606</td>\n",
              "      <td>0.370370</td>\n",
              "      <td>0.010904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2366</th>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.37500</td>\n",
              "      <td>0.313885</td>\n",
              "      <td>0.002664</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>0.320935</td>\n",
              "      <td>0.255187</td>\n",
              "      <td>0.943921</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>52</td>\n",
              "      <td>0.995412</td>\n",
              "      <td>1.007979</td>\n",
              "      <td>0.244767</td>\n",
              "      <td>0.005682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17431</th>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.40625</td>\n",
              "      <td>0.195716</td>\n",
              "      <td>0.004735</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>0.187035</td>\n",
              "      <td>0.184647</td>\n",
              "      <td>0.970720</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>31</td>\n",
              "      <td>0.993815</td>\n",
              "      <td>1.007229</td>\n",
              "      <td>0.386473</td>\n",
              "      <td>0.008869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5023</th>\n",
              "      <td>0.121212</td>\n",
              "      <td>0.31250</td>\n",
              "      <td>0.169129</td>\n",
              "      <td>0.002695</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.243358</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.994541</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>36</td>\n",
              "      <td>0.999182</td>\n",
              "      <td>1.007040</td>\n",
              "      <td>0.413849</td>\n",
              "      <td>0.005849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>749</th>\n",
              "      <td>0.121212</td>\n",
              "      <td>0.18750</td>\n",
              "      <td>0.111521</td>\n",
              "      <td>0.003912</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>0.113709</td>\n",
              "      <td>0.091286</td>\n",
              "      <td>0.977667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>22</td>\n",
              "      <td>0.998629</td>\n",
              "      <td>1.007410</td>\n",
              "      <td>0.233494</td>\n",
              "      <td>0.007610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2065</th>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.12500</td>\n",
              "      <td>0.091581</td>\n",
              "      <td>0.003149</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.461538</td>\n",
              "      <td>0.131775</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.977171</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>26</td>\n",
              "      <td>0.991471</td>\n",
              "      <td>1.006298</td>\n",
              "      <td>0.140097</td>\n",
              "      <td>0.005969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8413</th>\n",
              "      <td>0.121212</td>\n",
              "      <td>0.34375</td>\n",
              "      <td>0.228213</td>\n",
              "      <td>0.003997</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.191286</td>\n",
              "      <td>0.267635</td>\n",
              "      <td>0.970720</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>69</td>\n",
              "      <td>0.996992</td>\n",
              "      <td>1.008911</td>\n",
              "      <td>0.383253</td>\n",
              "      <td>0.005739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13231</th>\n",
              "      <td>0.060606</td>\n",
              "      <td>0.12500</td>\n",
              "      <td>0.097489</td>\n",
              "      <td>0.006036</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.461538</td>\n",
              "      <td>0.099894</td>\n",
              "      <td>0.078838</td>\n",
              "      <td>0.952357</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>46</td>\n",
              "      <td>0.995640</td>\n",
              "      <td>1.008086</td>\n",
              "      <td>0.317230</td>\n",
              "      <td>0.008709</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21110</th>\n",
              "      <td>0.060606</td>\n",
              "      <td>0.18750</td>\n",
              "      <td>0.085672</td>\n",
              "      <td>0.000677</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>0.108395</td>\n",
              "      <td>0.029046</td>\n",
              "      <td>0.996030</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>51</td>\n",
              "      <td>0.997861</td>\n",
              "      <td>1.008738</td>\n",
              "      <td>0.194847</td>\n",
              "      <td>0.001283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6229</th>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.21875</td>\n",
              "      <td>0.092688</td>\n",
              "      <td>0.000674</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>0.133369</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.997519</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>42</td>\n",
              "      <td>0.998288</td>\n",
              "      <td>1.008474</td>\n",
              "      <td>0.162641</td>\n",
              "      <td>0.001191</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11463</th>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.31250</td>\n",
              "      <td>0.129247</td>\n",
              "      <td>0.004072</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>0.141339</td>\n",
              "      <td>0.087137</td>\n",
              "      <td>0.989082</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>23</td>\n",
              "      <td>0.991092</td>\n",
              "      <td>1.006125</td>\n",
              "      <td>0.268921</td>\n",
              "      <td>0.008889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2715</th>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.12500</td>\n",
              "      <td>0.133678</td>\n",
              "      <td>0.007494</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>0.192349</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.977667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>36</td>\n",
              "      <td>0.999270</td>\n",
              "      <td>1.007064</td>\n",
              "      <td>0.228663</td>\n",
              "      <td>0.010740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18588</th>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.28125</td>\n",
              "      <td>0.115214</td>\n",
              "      <td>0.005190</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>0.114772</td>\n",
              "      <td>0.099585</td>\n",
              "      <td>0.981141</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3</td>\n",
              "      <td>0.996607</td>\n",
              "      <td>1.007377</td>\n",
              "      <td>0.428341</td>\n",
              "      <td>0.011043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6528</th>\n",
              "      <td>0.060606</td>\n",
              "      <td>0.21875</td>\n",
              "      <td>0.123338</td>\n",
              "      <td>0.002427</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.177471</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.995037</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>17</td>\n",
              "      <td>0.995318</td>\n",
              "      <td>1.005655</td>\n",
              "      <td>0.375201</td>\n",
              "      <td>0.004307</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16247</th>\n",
              "      <td>0.121212</td>\n",
              "      <td>0.21875</td>\n",
              "      <td>0.121861</td>\n",
              "      <td>0.004360</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>0.116897</td>\n",
              "      <td>0.114108</td>\n",
              "      <td>0.981141</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>19</td>\n",
              "      <td>0.991980</td>\n",
              "      <td>1.007188</td>\n",
              "      <td>0.260870</td>\n",
              "      <td>0.008464</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2693</th>\n",
              "      <td>0.121212</td>\n",
              "      <td>0.28125</td>\n",
              "      <td>0.129247</td>\n",
              "      <td>0.006153</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>0.140276</td>\n",
              "      <td>0.089212</td>\n",
              "      <td>0.978660</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4</td>\n",
              "      <td>0.996092</td>\n",
              "      <td>1.006949</td>\n",
              "      <td>0.349436</td>\n",
              "      <td>0.012816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8076</th>\n",
              "      <td>0.121212</td>\n",
              "      <td>0.25000</td>\n",
              "      <td>0.165436</td>\n",
              "      <td>0.004678</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>0.119022</td>\n",
              "      <td>0.232365</td>\n",
              "      <td>0.970720</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>44</td>\n",
              "      <td>0.994883</td>\n",
              "      <td>1.008655</td>\n",
              "      <td>0.215781</td>\n",
              "      <td>0.007231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20213</th>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.40625</td>\n",
              "      <td>0.107090</td>\n",
              "      <td>0.000889</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>0.116897</td>\n",
              "      <td>0.072614</td>\n",
              "      <td>0.997022</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>56</td>\n",
              "      <td>0.995580</td>\n",
              "      <td>1.008696</td>\n",
              "      <td>0.233494</td>\n",
              "      <td>0.001697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7624</th>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.12500</td>\n",
              "      <td>0.132201</td>\n",
              "      <td>0.007196</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>0.190223</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.968238</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>19</td>\n",
              "      <td>0.991860</td>\n",
              "      <td>1.007460</td>\n",
              "      <td>0.267311</td>\n",
              "      <td>0.013215</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>12967 rows × 18 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       bedrooms  bathrooms  sqft_living  ...      long  sqft_living15  sqft_lot15\n",
              "10952  0.060606    0.21875     0.158050  ...  1.008045       0.362319    0.004591\n",
              "1699   0.090909    0.25000     0.147710  ...  1.008886       0.296296    0.008035\n",
              "1721   0.090909    0.31250     0.120384  ...  1.007526       0.273752    0.010158\n",
              "15455  0.090909    0.21875     0.097489  ...  1.006718       0.296296    0.007536\n",
              "11423  0.090909    0.28125     0.135155  ...  1.007452       0.347826    0.009690\n",
              "291    0.121212    0.21875     0.217873  ...  1.007485       0.341385    0.011019\n",
              "18612  0.090909    0.31250     0.124815  ...  1.008779       0.138486    0.005896\n",
              "9146   0.151515    0.31250     0.180945  ...  1.007229       0.312399    0.006927\n",
              "13155  0.151515    0.34375     0.200886  ...  1.006644       0.454106    0.285000\n",
              "12071  0.090909    0.28125     0.128508  ...  1.007081       0.302738    0.009889\n",
              "6771   0.151515    0.34375     0.282939  ...  1.007715       0.619968    0.041969\n",
              "14809  0.121212    0.31250     0.168390  ...  1.008095       0.344605    0.011352\n",
              "17883  0.060606    0.12500     0.059084  ...  1.008194       0.148148    0.006818\n",
              "13362  0.090909    0.31250     0.204579  ...  1.006636       0.407407    0.011964\n",
              "11245  0.090909    0.21875     0.125554  ...  1.007097       0.272142    0.008500\n",
              "13415  0.121212    0.25000     0.094535  ...  1.007089       0.201288    0.008827\n",
              "14041  0.090909    0.31250     0.198671  ...  1.006372       0.581320    0.019384\n",
              "3548   0.121212    0.31250     0.233383  ...  1.006561       0.557166    0.019769\n",
              "11491  0.090909    0.12500     0.089365  ...  1.007839       0.246377    0.006772\n",
              "18039  0.121212    0.31250     0.155096  ...  1.005729       0.342995    0.009826\n",
              "14047  0.090909    0.18750     0.108567  ...  1.008614       0.281804    0.018450\n",
              "7361   0.090909    0.31250     0.183900  ...  1.005886       0.466989    0.039836\n",
              "20928  0.121212    0.31250     0.196455  ...  1.006100       0.384863    0.005597\n",
              "18581  0.090909    0.21875     0.105613  ...  1.006001       0.272142    0.015065\n",
              "638    0.090909    0.25000     0.151403  ...  1.008202       0.351047    0.013303\n",
              "6745   0.090909    0.28125     0.116691  ...  1.008589       0.278583    0.009241\n",
              "11292  0.090909    0.25000     0.169867  ...  1.009051       0.370370    0.005739\n",
              "8879   0.090909    0.21875     0.140251  ...  1.008119       0.322061    0.012000\n",
              "11627  0.090909    0.18750     0.115953  ...  1.008523       0.342995    0.014247\n",
              "14698  0.060606    0.18750     0.095273  ...  1.008037       0.207729    0.006205\n",
              "...         ...        ...          ...  ...       ...            ...         ...\n",
              "9551   0.121212    0.34375     0.164697  ...  1.008325       0.338164    0.011019\n",
              "9373   0.090909    0.21875     0.093796  ...  1.008202       0.204509    0.007382\n",
              "19388  0.090909    0.18750     0.093796  ...  1.008020       0.254428    0.005831\n",
              "17101  0.090909    0.12500     0.089365  ...  1.007122       0.254428    0.011150\n",
              "4908   0.151515    0.43750     0.258493  ...  1.005432       0.500805    0.012439\n",
              "19     0.090909    0.12500     0.092319  ...  1.008169       0.206119    0.010158\n",
              "10021  0.090909    0.31250     0.118168  ...  1.008894       0.223833    0.001516\n",
              "1063   0.090909    0.12500     0.075332  ...  1.006001       0.281804    0.013361\n",
              "13161  0.121212    0.31250     0.189069  ...  1.008144       0.283414    0.007328\n",
              "8388   0.090909    0.25000     0.127770  ...  1.006751       0.236715    0.010365\n",
              "8335   0.090909    0.31250     0.125554  ...  1.007468       0.276973    0.007122\n",
              "19210  0.151515    0.21875     0.143279  ...  1.008606       0.370370    0.010904\n",
              "2366   0.181818    0.37500     0.313885  ...  1.007979       0.244767    0.005682\n",
              "17431  0.090909    0.40625     0.195716  ...  1.007229       0.386473    0.008869\n",
              "5023   0.121212    0.31250     0.169129  ...  1.007040       0.413849    0.005849\n",
              "749    0.121212    0.18750     0.111521  ...  1.007410       0.233494    0.007610\n",
              "2065   0.090909    0.12500     0.091581  ...  1.006298       0.140097    0.005969\n",
              "8413   0.121212    0.34375     0.228213  ...  1.008911       0.383253    0.005739\n",
              "13231  0.060606    0.12500     0.097489  ...  1.008086       0.317230    0.008709\n",
              "21110  0.060606    0.18750     0.085672  ...  1.008738       0.194847    0.001283\n",
              "6229   0.090909    0.21875     0.092688  ...  1.008474       0.162641    0.001191\n",
              "11463  0.090909    0.31250     0.129247  ...  1.006125       0.268921    0.008889\n",
              "2715   0.090909    0.12500     0.133678  ...  1.007064       0.228663    0.010740\n",
              "18588  0.090909    0.28125     0.115214  ...  1.007377       0.428341    0.011043\n",
              "6528   0.060606    0.21875     0.123338  ...  1.005655       0.375201    0.004307\n",
              "16247  0.121212    0.21875     0.121861  ...  1.007188       0.260870    0.008464\n",
              "2693   0.121212    0.28125     0.129247  ...  1.006949       0.349436    0.012816\n",
              "8076   0.121212    0.25000     0.165436  ...  1.008655       0.215781    0.007231\n",
              "20213  0.090909    0.40625     0.107090  ...  1.008696       0.233494    0.001697\n",
              "7624   0.090909    0.12500     0.132201  ...  1.007460       0.267311    0.013215\n",
              "\n",
              "[12967 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6yaH3yFYAc7",
        "colab_type": "code",
        "outputId": "55762a4d-4258-48f8-e0b3-9fcfc4105aa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Model = 2 intermediate layers (100 neurons each) - Random Normal initializer\n",
        "ANN1 = Sequential()\n",
        "ANN1.add(Dense(100,kernel_initializer='random_normal', input_dim=len(Xtr.columns), activation= \"relu\"))\n",
        "ANN1.add(Dense(100,kernel_initializer='random_normal',  activation= \"relu\"))\n",
        "ANN1.add(Dense(1))\n",
        "ANN1.summary() #Print model Summary\n",
        "\n",
        "# Model Compilation\n",
        "ANN1.compile(loss= \"mean_squared_error\" , optimizer=\"adam\", metrics=[\"mean_squared_error\"])\n",
        "\n",
        "# Fit Model - 200 epochs\n",
        "ANN1.fit(Xtr, Ytr, epochs=200)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4409: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 100)               1900      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 12,101\n",
            "Trainable params: 12,101\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "Epoch 1/200\n",
            "12967/12967 [==============================] - 1s 81us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 2/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 3/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 4/200\n",
            "12967/12967 [==============================] - 0s 38us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 5/200\n",
            "12967/12967 [==============================] - 0s 39us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 6/200\n",
            "12967/12967 [==============================] - 0s 36us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 7/200\n",
            "12967/12967 [==============================] - 1s 39us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 8/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 9/200\n",
            "12967/12967 [==============================] - 0s 38us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 10/200\n",
            "12967/12967 [==============================] - 0s 38us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 11/200\n",
            "12967/12967 [==============================] - 1s 41us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 12/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 13/200\n",
            "12967/12967 [==============================] - 0s 36us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 14/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 15/200\n",
            "12967/12967 [==============================] - 0s 35us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 16/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 17/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 18/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 19/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 20/200\n",
            "12967/12967 [==============================] - 0s 36us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 21/200\n",
            "12967/12967 [==============================] - 0s 38us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 22/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 23/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 24/200\n",
            "12967/12967 [==============================] - 0s 36us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 25/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 26/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 27/200\n",
            "12967/12967 [==============================] - 0s 38us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 28/200\n",
            "12967/12967 [==============================] - 1s 39us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 29/200\n",
            "12967/12967 [==============================] - 0s 38us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 30/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 31/200\n",
            "12967/12967 [==============================] - 0s 39us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 32/200\n",
            "12967/12967 [==============================] - 0s 38us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 33/200\n",
            "12967/12967 [==============================] - 0s 38us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 34/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 35/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 36/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 37/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 38/200\n",
            "12967/12967 [==============================] - 0s 38us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 39/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 40/200\n",
            "12967/12967 [==============================] - 0s 36us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 41/200\n",
            "12967/12967 [==============================] - 0s 38us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 42/200\n",
            "12967/12967 [==============================] - 0s 38us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 43/200\n",
            "12967/12967 [==============================] - 0s 38us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 44/200\n",
            "12967/12967 [==============================] - 0s 36us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 45/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 46/200\n",
            "12967/12967 [==============================] - 0s 38us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 47/200\n",
            "12967/12967 [==============================] - 0s 38us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 48/200\n",
            "12967/12967 [==============================] - 0s 38us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 49/200\n",
            "12967/12967 [==============================] - 1s 40us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 50/200\n",
            "12967/12967 [==============================] - 0s 36us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 51/200\n",
            "12967/12967 [==============================] - 0s 38us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 52/200\n",
            "12967/12967 [==============================] - 1s 39us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 53/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 54/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 55/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 56/200\n",
            "12967/12967 [==============================] - 0s 36us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 57/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 58/200\n",
            "12967/12967 [==============================] - 0s 36us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 59/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 60/200\n",
            "12967/12967 [==============================] - 0s 36us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 61/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 62/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 63/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 64/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 65/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 66/200\n",
            "12967/12967 [==============================] - 0s 38us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 67/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 68/200\n",
            "12967/12967 [==============================] - 0s 38us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 69/200\n",
            "12967/12967 [==============================] - 0s 36us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 70/200\n",
            "12967/12967 [==============================] - 0s 38us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 71/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 72/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 73/200\n",
            "12967/12967 [==============================] - 0s 38us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 74/200\n",
            "12967/12967 [==============================] - 0s 36us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 75/200\n",
            "12967/12967 [==============================] - 1s 40us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 76/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 77/200\n",
            "12967/12967 [==============================] - 0s 36us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 78/200\n",
            "12967/12967 [==============================] - 0s 36us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 79/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 80/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 81/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 82/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 83/200\n",
            "12967/12967 [==============================] - 1s 41us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 84/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 85/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 86/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 87/200\n",
            "12967/12967 [==============================] - 1s 41us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 88/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 89/200\n",
            "12967/12967 [==============================] - 0s 38us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 90/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 91/200\n",
            "12967/12967 [==============================] - 1s 39us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 92/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 93/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 94/200\n",
            "12967/12967 [==============================] - 0s 36us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 95/200\n",
            "12967/12967 [==============================] - 0s 36us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 96/200\n",
            "12967/12967 [==============================] - 0s 38us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 97/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 98/200\n",
            "12967/12967 [==============================] - 0s 36us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 99/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 100/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 101/200\n",
            "12967/12967 [==============================] - 0s 38us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 102/200\n",
            "12967/12967 [==============================] - 0s 36us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 103/200\n",
            "12967/12967 [==============================] - 0s 36us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 104/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 105/200\n",
            "12967/12967 [==============================] - 0s 38us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 106/200\n",
            "12967/12967 [==============================] - 0s 36us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 107/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 108/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 109/200\n",
            "12967/12967 [==============================] - 0s 38us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 110/200\n",
            "12967/12967 [==============================] - 0s 36us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 111/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 112/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 113/200\n",
            "12967/12967 [==============================] - 1s 39us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 114/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 115/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 116/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 117/200\n",
            "12967/12967 [==============================] - 0s 38us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 118/200\n",
            "12967/12967 [==============================] - 0s 38us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 119/200\n",
            "12967/12967 [==============================] - 0s 38us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 120/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 121/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 122/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 123/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 124/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 125/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 126/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 127/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 128/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 129/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 130/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 131/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 132/200\n",
            "12967/12967 [==============================] - 0s 39us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 133/200\n",
            "12967/12967 [==============================] - 0s 36us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 134/200\n",
            "12967/12967 [==============================] - 0s 38us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 135/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 136/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 137/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 138/200\n",
            "12967/12967 [==============================] - 0s 36us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 139/200\n",
            "12967/12967 [==============================] - 0s 36us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 140/200\n",
            "12967/12967 [==============================] - 0s 38us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 141/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 142/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 143/200\n",
            "12967/12967 [==============================] - 0s 36us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 144/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 145/200\n",
            "12967/12967 [==============================] - 0s 36us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 146/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 147/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 148/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 149/200\n",
            "12967/12967 [==============================] - 0s 36us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 150/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 151/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 152/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 153/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 154/200\n",
            "12967/12967 [==============================] - 0s 36us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 155/200\n",
            "12967/12967 [==============================] - 0s 38us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 156/200\n",
            "12967/12967 [==============================] - 0s 38us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 157/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 158/200\n",
            "12967/12967 [==============================] - 0s 36us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 159/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 160/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 161/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 162/200\n",
            "12967/12967 [==============================] - 0s 35us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 163/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 164/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 165/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 166/200\n",
            "12967/12967 [==============================] - 1s 39us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 167/200\n",
            "12967/12967 [==============================] - 0s 38us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 168/200\n",
            "12967/12967 [==============================] - 0s 36us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 169/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 170/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 171/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 172/200\n",
            "12967/12967 [==============================] - 0s 38us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 173/200\n",
            "12967/12967 [==============================] - 0s 36us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 174/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 175/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 176/200\n",
            "12967/12967 [==============================] - 0s 38us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 177/200\n",
            "12967/12967 [==============================] - 0s 38us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 178/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 179/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 180/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 181/200\n",
            "12967/12967 [==============================] - 0s 36us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 182/200\n",
            "12967/12967 [==============================] - 0s 36us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 183/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 184/200\n",
            "12967/12967 [==============================] - 0s 36us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 185/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 186/200\n",
            "12967/12967 [==============================] - 0s 38us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 187/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 188/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 189/200\n",
            "12967/12967 [==============================] - 0s 36us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 190/200\n",
            "12967/12967 [==============================] - 0s 38us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 191/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 192/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 193/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 194/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 195/200\n",
            "12967/12967 [==============================] - 0s 35us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 196/200\n",
            "12967/12967 [==============================] - 0s 36us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 197/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 198/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 199/200\n",
            "12967/12967 [==============================] - 0s 36us/step - loss: nan - mean_squared_error: nan\n",
            "Epoch 200/200\n",
            "12967/12967 [==============================] - 0s 37us/step - loss: nan - mean_squared_error: nan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7c3ae87ba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwEuX5FiZALN",
        "colab_type": "code",
        "outputId": "3756b316-cb41-449e-d889-faee2d3ea895",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "source": [
        "#12967/12967 [==============================] - 1s 41us/step - loss: 44003656324.0768 - mean_squared_error: 44003656324.0768\n",
        "#<keras.callbacks.History at 0x7f1e5fd66978>\n",
        "ANN1_pred = ANN1.predict(Xva)\n",
        "score = np.sqrt(mean_squared_error(Yva,ANN1_pred))\n",
        "\n",
        "for i in np.arange(0,5):\n",
        "  print(f'yhat={ANN1_pred[i]} y= {Yva.iloc[i]}')\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-034e14507030>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mANN1_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mANN1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXva\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYva\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mANN1_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'yhat={ANN1_pred[i]} y= {Yva.iloc[i]}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/regression.py\u001b[0m in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \"\"\"\n\u001b[1;32m    240\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0;32m--> 241\u001b[0;31m         y_true, y_pred, multioutput)\n\u001b[0m\u001b[1;32m    242\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 542\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m     55\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oK8pJlaRbPzP",
        "colab_type": "code",
        "outputId": "98ddf0fb-ce66-484f-a90a-1b2865989886",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(score)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "199067.01481365509\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAuSidn9boHp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FA6XFRAPbtTB",
        "colab_type": "code",
        "outputId": "487137f5-59db-406d-a663-9940d7e5b51a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Model #2 = 3 intermediate layers (100 neurons each) - Random Initializer\n",
        "ANN2 = Sequential()\n",
        "ANN2.add(Dense(100,kernel_initializer='random_normal', input_dim=len(Xtr.columns), activation= \"relu\"))\n",
        "ANN2.add(Dense(100,kernel_initializer='random_normal',  activation= \"relu\"))\n",
        "ANN2.add(Dense(100,kernel_initializer='random_normal',  activation= \"relu\"))\n",
        "ANN2.add(Dense(1))\n",
        "ANN2.summary() #Print model Summary\n",
        "\n",
        "# Model Compilation\n",
        "ANN2.compile(loss= \"mean_squared_error\" , optimizer=\"adam\", metrics=[\"mean_squared_error\"])\n",
        "\n",
        "# Fit Model - 50 epochs\n",
        "ANN2.fit(Xtr, Ytr, epochs=50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_23 (Dense)             (None, 100)               1900      \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 22,201\n",
            "Trainable params: 22,201\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "12967/12967 [==============================] - 1s 90us/step - loss: 294940127104.0691 - mean_squared_error: 294940127104.0691\n",
            "Epoch 2/50\n",
            "12967/12967 [==============================] - 1s 46us/step - loss: 165399604378.7016 - mean_squared_error: 165399604378.7016\n",
            "Epoch 3/50\n",
            "12967/12967 [==============================] - 1s 46us/step - loss: 130882209901.4520 - mean_squared_error: 130882209901.4520\n",
            "Epoch 4/50\n",
            "12967/12967 [==============================] - 1s 46us/step - loss: 101155312921.5270 - mean_squared_error: 101155312921.5270\n",
            "Epoch 5/50\n",
            "12967/12967 [==============================] - 1s 46us/step - loss: 94624440508.8955 - mean_squared_error: 94624440508.8955\n",
            "Epoch 6/50\n",
            "12967/12967 [==============================] - 1s 46us/step - loss: 89669813828.1903 - mean_squared_error: 89669813828.1903\n",
            "Epoch 7/50\n",
            "12967/12967 [==============================] - 1s 45us/step - loss: 84730868834.0014 - mean_squared_error: 84730868834.0014\n",
            "Epoch 8/50\n",
            "12967/12967 [==============================] - 1s 46us/step - loss: 79579247602.4962 - mean_squared_error: 79579247602.4962\n",
            "Epoch 9/50\n",
            "12967/12967 [==============================] - 1s 46us/step - loss: 74412419844.7974 - mean_squared_error: 74412419844.7974\n",
            "Epoch 10/50\n",
            "12967/12967 [==============================] - 1s 46us/step - loss: 69484856580.9948 - mean_squared_error: 69484856580.9948\n",
            "Epoch 11/50\n",
            "12967/12967 [==============================] - 1s 46us/step - loss: 65508144572.5204 - mean_squared_error: 65508144572.5204\n",
            "Epoch 12/50\n",
            "12967/12967 [==============================] - 1s 46us/step - loss: 62707309335.5922 - mean_squared_error: 62707309335.5922\n",
            "Epoch 13/50\n",
            "12967/12967 [==============================] - 1s 45us/step - loss: 60462481289.2296 - mean_squared_error: 60462481289.2296\n",
            "Epoch 14/50\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 58627807753.2000 - mean_squared_error: 58627807753.2000\n",
            "Epoch 15/50\n",
            "12967/12967 [==============================] - 1s 46us/step - loss: 57198670799.1178 - mean_squared_error: 57198670799.1178\n",
            "Epoch 16/50\n",
            "12967/12967 [==============================] - 1s 47us/step - loss: 56124541843.0218 - mean_squared_error: 56124541843.0218\n",
            "Epoch 17/50\n",
            "12967/12967 [==============================] - 1s 47us/step - loss: 55355558366.2405 - mean_squared_error: 55355558366.2405\n",
            "Epoch 18/50\n",
            "12967/12967 [==============================] - 1s 47us/step - loss: 54604929285.5476 - mean_squared_error: 54604929285.5476\n",
            "Epoch 19/50\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 53844742716.6882 - mean_squared_error: 53844742716.6882\n",
            "Epoch 20/50\n",
            "12967/12967 [==============================] - 1s 46us/step - loss: 52998585513.8638 - mean_squared_error: 52998585513.8638\n",
            "Epoch 21/50\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 52480781694.9240 - mean_squared_error: 52480781694.9240\n",
            "Epoch 22/50\n",
            "12967/12967 [==============================] - 1s 48us/step - loss: 51812414002.9354 - mean_squared_error: 51812414002.9354\n",
            "Epoch 23/50\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 51280221744.1320 - mean_squared_error: 51280221744.1320\n",
            "Epoch 24/50\n",
            "12967/12967 [==============================] - 1s 47us/step - loss: 50767728113.4301 - mean_squared_error: 50767728113.4301\n",
            "Epoch 25/50\n",
            "12967/12967 [==============================] - 1s 47us/step - loss: 50385582111.3510 - mean_squared_error: 50385582111.3510\n",
            "Epoch 26/50\n",
            "12967/12967 [==============================] - 1s 47us/step - loss: 49867461106.1408 - mean_squared_error: 49867461106.1408\n",
            "Epoch 27/50\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 49469471252.0978 - mean_squared_error: 49469471252.0978\n",
            "Epoch 28/50\n",
            "12967/12967 [==============================] - 1s 47us/step - loss: 49058482434.8626 - mean_squared_error: 49058482434.8626\n",
            "Epoch 29/50\n",
            "12967/12967 [==============================] - 1s 47us/step - loss: 48673558111.6718 - mean_squared_error: 48673558111.6718\n",
            "Epoch 30/50\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 48507455007.0746 - mean_squared_error: 48507455007.0746\n",
            "Epoch 31/50\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 48073440324.7826 - mean_squared_error: 48073440324.7826\n",
            "Epoch 32/50\n",
            "12967/12967 [==============================] - 1s 47us/step - loss: 47692551383.8242 - mean_squared_error: 47692551383.8242\n",
            "Epoch 33/50\n",
            "12967/12967 [==============================] - 1s 48us/step - loss: 47562118501.2589 - mean_squared_error: 47562118501.2589\n",
            "Epoch 34/50\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 47225114927.4016 - mean_squared_error: 47225114927.4016\n",
            "Epoch 35/50\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 46830018328.3029 - mean_squared_error: 46830018328.3029\n",
            "Epoch 36/50\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 46723985579.1668 - mean_squared_error: 46723985579.1668\n",
            "Epoch 37/50\n",
            "12967/12967 [==============================] - 1s 48us/step - loss: 46425739919.4484 - mean_squared_error: 46425739919.4484\n",
            "Epoch 38/50\n",
            "12967/12967 [==============================] - 1s 48us/step - loss: 46140440632.8582 - mean_squared_error: 46140440632.8582\n",
            "Epoch 39/50\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 46015119409.6719 - mean_squared_error: 46015119409.6719\n",
            "Epoch 40/50\n",
            "12967/12967 [==============================] - 1s 48us/step - loss: 45897675856.3122 - mean_squared_error: 45897675856.3122\n",
            "Epoch 41/50\n",
            "12967/12967 [==============================] - 1s 48us/step - loss: 45759584596.0435 - mean_squared_error: 45759584596.0435\n",
            "Epoch 42/50\n",
            "12967/12967 [==============================] - 1s 47us/step - loss: 45568686300.2465 - mean_squared_error: 45568686300.2465\n",
            "Epoch 43/50\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 45281213201.3536 - mean_squared_error: 45281213201.3536\n",
            "Epoch 44/50\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 45203223116.7980 - mean_squared_error: 45203223116.7980\n",
            "Epoch 45/50\n",
            "12967/12967 [==============================] - 1s 47us/step - loss: 45086534697.6170 - mean_squared_error: 45086534697.6170\n",
            "Epoch 46/50\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 45085344030.1862 - mean_squared_error: 45085344030.1862\n",
            "Epoch 47/50\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 44910858365.7987 - mean_squared_error: 44910858365.7987\n",
            "Epoch 48/50\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 44647154755.2032 - mean_squared_error: 44647154755.2032\n",
            "Epoch 49/50\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 44625158233.1568 - mean_squared_error: 44625158233.1568\n",
            "Epoch 50/50\n",
            "12967/12967 [==============================] - 1s 48us/step - loss: 44514573330.8738 - mean_squared_error: 44514573330.8738\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1e5f9d0e80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-4Yq7wJbuYH",
        "colab_type": "code",
        "outputId": "7986a4ac-3791-4dd3-a4ce-d08876cd7377",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "#12967/12967 [==============================] - 1s 48us/step - loss: 44514573330.8738 - mean_squared_error: 44514573330.8738\n",
        "#<keras.callbacks.History at 0x7f1e5f9d0e80>\n",
        "\n",
        "ANN2_pred = ANN2.predict(Xva)\n",
        "score = np.sqrt(mean_squared_error(Yva,ANN2_pred))\n",
        "\n",
        "for i in np.arange(0,5):\n",
        "  print(f'yhat={ANN2_pred[i]} y= {Yva.iloc[i]}')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "yhat=[454807.4] y= 440000.0\n",
            "yhat=[532454.75] y= 278226.0\n",
            "yhat=[434630.5] y= 304000.0\n",
            "yhat=[369783.34] y= 265000.0\n",
            "yhat=[337944.25] y= 475000.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TbGZbzebx70",
        "colab_type": "code",
        "outputId": "67fa40d8-dcf8-4bac-ccfd-93795cd21e91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Model #3 = 3 intermediate layers (100 neurons each) - Random initializer\n",
        "ANN3 = Sequential()\n",
        "ANN3.add(Dense(100,kernel_initializer='random_normal', input_dim=len(Xtr.columns), activation= \"relu\"))\n",
        "ANN3.add(Dense(100,kernel_initializer='random_normal',  activation= \"relu\"))\n",
        "ANN3.add(Dense(100,kernel_initializer='random_normal',  activation= \"relu\"))\n",
        "ANN3.add(Dense(1))\n",
        "ANN3.summary() #Print model Summary\n",
        "\n",
        "# Model Compilation\n",
        "ANN3.compile(loss= \"mean_squared_error\" , optimizer=\"adam\", metrics=[\"mean_squared_error\"])\n",
        "\n",
        "# Fit Model - 200 epochs\n",
        "ANN3.fit(Xtr, Ytr, epochs=200)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_27 (Dense)             (None, 100)               1900      \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 22,201\n",
            "Trainable params: 22,201\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "12967/12967 [==============================] - 1s 98us/step - loss: 293806867351.8390 - mean_squared_error: 293806867351.8390\n",
            "Epoch 2/200\n",
            "12967/12967 [==============================] - 1s 48us/step - loss: 167386455172.5901 - mean_squared_error: 167386455172.5901\n",
            "Epoch 3/200\n",
            "12967/12967 [==============================] - 1s 48us/step - loss: 137087745199.2337 - mean_squared_error: 137087745199.2337\n",
            "Epoch 4/200\n",
            "12967/12967 [==============================] - 1s 47us/step - loss: 104366354476.5389 - mean_squared_error: 104366354476.5389\n",
            "Epoch 5/200\n",
            "12967/12967 [==============================] - 1s 47us/step - loss: 95509288277.7019 - mean_squared_error: 95509288277.7019\n",
            "Epoch 6/200\n",
            "12967/12967 [==============================] - 1s 48us/step - loss: 90802111438.1701 - mean_squared_error: 90802111438.1701\n",
            "Epoch 7/200\n",
            "12967/12967 [==============================] - 1s 47us/step - loss: 85901989188.4865 - mean_squared_error: 85901989188.4865\n",
            "Epoch 8/200\n",
            "12967/12967 [==============================] - 1s 47us/step - loss: 80756698133.7167 - mean_squared_error: 80756698133.7167\n",
            "Epoch 9/200\n",
            "12967/12967 [==============================] - 1s 46us/step - loss: 75520988003.4031 - mean_squared_error: 75520988003.4031\n",
            "Epoch 10/200\n",
            "12967/12967 [==============================] - 1s 47us/step - loss: 70583846709.6007 - mean_squared_error: 70583846709.6007\n",
            "Epoch 11/200\n",
            "12967/12967 [==============================] - 1s 46us/step - loss: 66201810641.3881 - mean_squared_error: 66201810641.3881\n",
            "Epoch 12/200\n",
            "12967/12967 [==============================] - 1s 47us/step - loss: 63105968358.9074 - mean_squared_error: 63105968358.9074\n",
            "Epoch 13/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 60945154012.9375 - mean_squared_error: 60945154012.9375\n",
            "Epoch 14/200\n",
            "12967/12967 [==============================] - 1s 48us/step - loss: 59151328699.5728 - mean_squared_error: 59151328699.5728\n",
            "Epoch 15/200\n",
            "12967/12967 [==============================] - 1s 47us/step - loss: 57596636836.8492 - mean_squared_error: 57596636836.8492\n",
            "Epoch 16/200\n",
            "12967/12967 [==============================] - 1s 46us/step - loss: 56603086697.4838 - mean_squared_error: 56603086697.4838\n",
            "Epoch 17/200\n",
            "12967/12967 [==============================] - 1s 48us/step - loss: 55592481466.7238 - mean_squared_error: 55592481466.7238\n",
            "Epoch 18/200\n",
            "12967/12967 [==============================] - 1s 48us/step - loss: 54687487749.2712 - mean_squared_error: 54687487749.2712\n",
            "Epoch 19/200\n",
            "12967/12967 [==============================] - 1s 46us/step - loss: 54019032900.9998 - mean_squared_error: 54019032900.9998\n",
            "Epoch 20/200\n",
            "12967/12967 [==============================] - 1s 47us/step - loss: 53318794033.8101 - mean_squared_error: 53318794033.8101\n",
            "Epoch 21/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 52782917165.2891 - mean_squared_error: 52782917165.2891\n",
            "Epoch 22/200\n",
            "12967/12967 [==============================] - 1s 46us/step - loss: 52202133173.1170 - mean_squared_error: 52202133173.1170\n",
            "Epoch 23/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 51588304179.9028 - mean_squared_error: 51588304179.9028\n",
            "Epoch 24/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 51061556751.9914 - mean_squared_error: 51061556751.9914\n",
            "Epoch 25/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 50654296976.4158 - mean_squared_error: 50654296976.4158\n",
            "Epoch 26/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 50220678202.6745 - mean_squared_error: 50220678202.6745\n",
            "Epoch 27/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 49725010442.4635 - mean_squared_error: 49725010442.4635\n",
            "Epoch 28/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 49375440688.7835 - mean_squared_error: 49375440688.7835\n",
            "Epoch 29/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 49045473916.4957 - mean_squared_error: 49045473916.4957\n",
            "Epoch 30/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 48708593204.1200 - mean_squared_error: 48708593204.1200\n",
            "Epoch 31/200\n",
            "12967/12967 [==============================] - 1s 57us/step - loss: 48317713887.8988 - mean_squared_error: 48317713887.8988\n",
            "Epoch 32/200\n",
            "12967/12967 [==============================] - 1s 56us/step - loss: 47962491388.8807 - mean_squared_error: 47962491388.8807\n",
            "Epoch 33/200\n",
            "12967/12967 [==============================] - 1s 57us/step - loss: 47775754870.0202 - mean_squared_error: 47775754870.0202\n",
            "Epoch 34/200\n",
            "12967/12967 [==============================] - 1s 58us/step - loss: 47440495139.6153 - mean_squared_error: 47440495139.6153\n",
            "Epoch 35/200\n",
            "12967/12967 [==============================] - 1s 62us/step - loss: 47168855303.3244 - mean_squared_error: 47168855303.3244\n",
            "Epoch 36/200\n",
            "12967/12967 [==============================] - 1s 60us/step - loss: 46980957960.5090 - mean_squared_error: 46980957960.5090\n",
            "Epoch 37/200\n",
            "12967/12967 [==============================] - 1s 63us/step - loss: 46751251053.4915 - mean_squared_error: 46751251053.4915\n",
            "Epoch 38/200\n",
            "12967/12967 [==============================] - 1s 61us/step - loss: 46515861536.4566 - mean_squared_error: 46515861536.4566\n",
            "Epoch 39/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 46319072993.0241 - mean_squared_error: 46319072993.0241\n",
            "Epoch 40/200\n",
            "12967/12967 [==============================] - 1s 48us/step - loss: 46060887984.7144 - mean_squared_error: 46060887984.7144\n",
            "Epoch 41/200\n",
            "12967/12967 [==============================] - 1s 47us/step - loss: 45921248925.5840 - mean_squared_error: 45921248925.5840\n",
            "Epoch 42/200\n",
            "12967/12967 [==============================] - 1s 48us/step - loss: 45734088188.3279 - mean_squared_error: 45734088188.3279\n",
            "Epoch 43/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 45519853836.4970 - mean_squared_error: 45519853836.4970\n",
            "Epoch 44/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 45473193188.6173 - mean_squared_error: 45473193188.6173\n",
            "Epoch 45/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 45287876622.4515 - mean_squared_error: 45287876622.4515\n",
            "Epoch 46/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 45166622182.2164 - mean_squared_error: 45166622182.2164\n",
            "Epoch 47/200\n",
            "12967/12967 [==============================] - 1s 48us/step - loss: 45010757548.1342 - mean_squared_error: 45010757548.1342\n",
            "Epoch 48/200\n",
            "12967/12967 [==============================] - 1s 48us/step - loss: 44943477173.3342 - mean_squared_error: 44943477173.3342\n",
            "Epoch 49/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 44826918216.5929 - mean_squared_error: 44826918216.5929\n",
            "Epoch 50/200\n",
            "12967/12967 [==============================] - 1s 47us/step - loss: 44662004862.5884 - mean_squared_error: 44662004862.5884\n",
            "Epoch 51/200\n",
            "12967/12967 [==============================] - 1s 47us/step - loss: 44625223448.9347 - mean_squared_error: 44625223448.9347\n",
            "Epoch 52/200\n",
            "12967/12967 [==============================] - 1s 47us/step - loss: 44619049270.3509 - mean_squared_error: 44619049270.3509\n",
            "Epoch 53/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 44436445934.6859 - mean_squared_error: 44436445934.6859\n",
            "Epoch 54/200\n",
            "12967/12967 [==============================] - 1s 47us/step - loss: 44299232931.9016 - mean_squared_error: 44299232931.9016\n",
            "Epoch 55/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 44092688001.1549 - mean_squared_error: 44092688001.1549\n",
            "Epoch 56/200\n",
            "12967/12967 [==============================] - 1s 47us/step - loss: 44139842617.6479 - mean_squared_error: 44139842617.6479\n",
            "Epoch 57/200\n",
            "12967/12967 [==============================] - 1s 48us/step - loss: 44085035740.5426 - mean_squared_error: 44085035740.5426\n",
            "Epoch 58/200\n",
            "12967/12967 [==============================] - 1s 47us/step - loss: 43943826937.0112 - mean_squared_error: 43943826937.0112\n",
            "Epoch 59/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 43789686248.7434 - mean_squared_error: 43789686248.7434\n",
            "Epoch 60/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 43772383145.8835 - mean_squared_error: 43772383145.8835\n",
            "Epoch 61/200\n",
            "12967/12967 [==============================] - 1s 48us/step - loss: 43603588394.4265 - mean_squared_error: 43603588394.4265\n",
            "Epoch 62/200\n",
            "12967/12967 [==============================] - 1s 48us/step - loss: 43449860946.0298 - mean_squared_error: 43449860946.0298\n",
            "Epoch 63/200\n",
            "12967/12967 [==============================] - 1s 48us/step - loss: 43474783454.1417 - mean_squared_error: 43474783454.1417\n",
            "Epoch 64/200\n",
            "12967/12967 [==============================] - 1s 46us/step - loss: 43348964290.4826 - mean_squared_error: 43348964290.4826\n",
            "Epoch 65/200\n",
            "12967/12967 [==============================] - 1s 48us/step - loss: 43315911466.8608 - mean_squared_error: 43315911466.8608\n",
            "Epoch 66/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 43218497514.5202 - mean_squared_error: 43218497514.5202\n",
            "Epoch 67/200\n",
            "12967/12967 [==============================] - 1s 47us/step - loss: 43137728023.3355 - mean_squared_error: 43137728023.3355\n",
            "Epoch 68/200\n",
            "12967/12967 [==============================] - 1s 46us/step - loss: 43054404661.4625 - mean_squared_error: 43054404661.4625\n",
            "Epoch 69/200\n",
            "12967/12967 [==============================] - 1s 47us/step - loss: 42887746887.6452 - mean_squared_error: 42887746887.6452\n",
            "Epoch 70/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 42981500311.5626 - mean_squared_error: 42981500311.5626\n",
            "Epoch 71/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 42864267928.9248 - mean_squared_error: 42864267928.9248\n",
            "Epoch 72/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 42685286291.4956 - mean_squared_error: 42685286291.4956\n",
            "Epoch 73/200\n",
            "12967/12967 [==============================] - 1s 46us/step - loss: 42690357272.4806 - mean_squared_error: 42690357272.4806\n",
            "Epoch 74/200\n",
            "12967/12967 [==============================] - 1s 47us/step - loss: 42618670875.6986 - mean_squared_error: 42618670875.6986\n",
            "Epoch 75/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 42468518000.9267 - mean_squared_error: 42468518000.9267\n",
            "Epoch 76/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 42423427313.8052 - mean_squared_error: 42423427313.8052\n",
            "Epoch 77/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 42390849408.8588 - mean_squared_error: 42390849408.8588\n",
            "Epoch 78/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 42360329237.8746 - mean_squared_error: 42360329237.8746\n",
            "Epoch 79/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 42334332335.8852 - mean_squared_error: 42334332335.8852\n",
            "Epoch 80/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 42264401196.3217 - mean_squared_error: 42264401196.3217\n",
            "Epoch 81/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 42078358613.3662 - mean_squared_error: 42078358613.3662\n",
            "Epoch 82/200\n",
            "12967/12967 [==============================] - 1s 48us/step - loss: 42031199609.4751 - mean_squared_error: 42031199609.4751\n",
            "Epoch 83/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 42123856522.3944 - mean_squared_error: 42123856522.3944\n",
            "Epoch 84/200\n",
            "12967/12967 [==============================] - 1s 47us/step - loss: 42002324249.5664 - mean_squared_error: 42002324249.5664\n",
            "Epoch 85/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 41803030245.2885 - mean_squared_error: 41803030245.2885\n",
            "Epoch 86/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 41876671547.7801 - mean_squared_error: 41876671547.7801\n",
            "Epoch 87/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 41756651556.7999 - mean_squared_error: 41756651556.7999\n",
            "Epoch 88/200\n",
            "12967/12967 [==============================] - 1s 47us/step - loss: 41810736455.3294 - mean_squared_error: 41810736455.3294\n",
            "Epoch 89/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 41687502047.7211 - mean_squared_error: 41687502047.7211\n",
            "Epoch 90/200\n",
            "12967/12967 [==============================] - 1s 48us/step - loss: 41681721818.2130 - mean_squared_error: 41681721818.2130\n",
            "Epoch 91/200\n",
            "12967/12967 [==============================] - 1s 46us/step - loss: 41538151077.0861 - mean_squared_error: 41538151077.0861\n",
            "Epoch 92/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 41424887235.6277 - mean_squared_error: 41424887235.6277\n",
            "Epoch 93/200\n",
            "12967/12967 [==============================] - 1s 55us/step - loss: 41495508119.2270 - mean_squared_error: 41495508119.2270\n",
            "Epoch 94/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 41407253094.2263 - mean_squared_error: 41407253094.2263\n",
            "Epoch 95/200\n",
            "12967/12967 [==============================] - 1s 48us/step - loss: 41252322307.8695 - mean_squared_error: 41252322307.8695\n",
            "Epoch 96/200\n",
            "12967/12967 [==============================] - 1s 47us/step - loss: 41212838248.7336 - mean_squared_error: 41212838248.7336\n",
            "Epoch 97/200\n",
            "12967/12967 [==============================] - 1s 46us/step - loss: 41263323433.3999 - mean_squared_error: 41263323433.3999\n",
            "Epoch 98/200\n",
            "12967/12967 [==============================] - 1s 46us/step - loss: 41209746511.9963 - mean_squared_error: 41209746511.9963\n",
            "Epoch 99/200\n",
            "12967/12967 [==============================] - 1s 46us/step - loss: 41182617426.8984 - mean_squared_error: 41182617426.8984\n",
            "Epoch 100/200\n",
            "12967/12967 [==============================] - 1s 47us/step - loss: 41098711739.8294 - mean_squared_error: 41098711739.8294\n",
            "Epoch 101/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 41060495635.7622 - mean_squared_error: 41060495635.7622\n",
            "Epoch 102/200\n",
            "12967/12967 [==============================] - 1s 48us/step - loss: 40901465819.8121 - mean_squared_error: 40901465819.8121\n",
            "Epoch 103/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 40882848025.0531 - mean_squared_error: 40882848025.0531\n",
            "Epoch 104/200\n",
            "12967/12967 [==============================] - 1s 48us/step - loss: 40877038294.9950 - mean_squared_error: 40877038294.9950\n",
            "Epoch 105/200\n",
            "12967/12967 [==============================] - 1s 48us/step - loss: 40796904025.9070 - mean_squared_error: 40796904025.9070\n",
            "Epoch 106/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 40805191631.9864 - mean_squared_error: 40805191631.9864\n",
            "Epoch 107/200\n",
            "12967/12967 [==============================] - 1s 48us/step - loss: 40704520852.5025 - mean_squared_error: 40704520852.5025\n",
            "Epoch 108/200\n",
            "12967/12967 [==============================] - 1s 54us/step - loss: 40770302150.0560 - mean_squared_error: 40770302150.0560\n",
            "Epoch 109/200\n",
            "12967/12967 [==============================] - 1s 46us/step - loss: 40691120787.7128 - mean_squared_error: 40691120787.7128\n",
            "Epoch 110/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 40581553052.0244 - mean_squared_error: 40581553052.0244\n",
            "Epoch 111/200\n",
            "12967/12967 [==============================] - 1s 48us/step - loss: 40511760153.4875 - mean_squared_error: 40511760153.4875\n",
            "Epoch 112/200\n",
            "12967/12967 [==============================] - 1s 48us/step - loss: 40444013915.0718 - mean_squared_error: 40444013915.0718\n",
            "Epoch 113/200\n",
            "12967/12967 [==============================] - 1s 48us/step - loss: 40498311616.1135 - mean_squared_error: 40498311616.1135\n",
            "Epoch 114/200\n",
            "12967/12967 [==============================] - 1s 48us/step - loss: 40380780666.2056 - mean_squared_error: 40380780666.2056\n",
            "Epoch 115/200\n",
            "12967/12967 [==============================] - 1s 48us/step - loss: 40355847423.3090 - mean_squared_error: 40355847423.3090\n",
            "Epoch 116/200\n",
            "12967/12967 [==============================] - 1s 46us/step - loss: 40318203799.6021 - mean_squared_error: 40318203799.6021\n",
            "Epoch 117/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 40419974036.5222 - mean_squared_error: 40419974036.5222\n",
            "Epoch 118/200\n",
            "12967/12967 [==============================] - 1s 48us/step - loss: 40264174862.9450 - mean_squared_error: 40264174862.9450\n",
            "Epoch 119/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 40227251665.2105 - mean_squared_error: 40227251665.2105\n",
            "Epoch 120/200\n",
            "12967/12967 [==============================] - 1s 47us/step - loss: 40181145613.5828 - mean_squared_error: 40181145613.5828\n",
            "Epoch 121/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 40115114508.9905 - mean_squared_error: 40115114508.9905\n",
            "Epoch 122/200\n",
            "12967/12967 [==============================] - 1s 46us/step - loss: 40165531074.8775 - mean_squared_error: 40165531074.8775\n",
            "Epoch 123/200\n",
            "12967/12967 [==============================] - 1s 47us/step - loss: 40082304345.0186 - mean_squared_error: 40082304345.0186\n",
            "Epoch 124/200\n",
            "12967/12967 [==============================] - 1s 46us/step - loss: 39985881845.3983 - mean_squared_error: 39985881845.3983\n",
            "Epoch 125/200\n",
            "12967/12967 [==============================] - 1s 47us/step - loss: 40055944649.7873 - mean_squared_error: 40055944649.7873\n",
            "Epoch 126/200\n",
            "12967/12967 [==============================] - 1s 46us/step - loss: 40099385330.4172 - mean_squared_error: 40099385330.4172\n",
            "Epoch 127/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 39948977430.6446 - mean_squared_error: 39948977430.6446\n",
            "Epoch 128/200\n",
            "12967/12967 [==============================] - 1s 45us/step - loss: 40020297710.3108 - mean_squared_error: 40020297710.3108\n",
            "Epoch 129/200\n",
            "12967/12967 [==============================] - 1s 48us/step - loss: 39825224653.8542 - mean_squared_error: 39825224653.8542\n",
            "Epoch 130/200\n",
            "12967/12967 [==============================] - 1s 48us/step - loss: 39737250935.6391 - mean_squared_error: 39737250935.6391\n",
            "Epoch 131/200\n",
            "12967/12967 [==============================] - 1s 48us/step - loss: 39694047645.6432 - mean_squared_error: 39694047645.6432\n",
            "Epoch 132/200\n",
            "12967/12967 [==============================] - 1s 46us/step - loss: 39811589801.8638 - mean_squared_error: 39811589801.8638\n",
            "Epoch 133/200\n",
            "12967/12967 [==============================] - 1s 47us/step - loss: 39579883238.0782 - mean_squared_error: 39579883238.0782\n",
            "Epoch 134/200\n",
            "12967/12967 [==============================] - 1s 47us/step - loss: 39773157407.7458 - mean_squared_error: 39773157407.7458\n",
            "Epoch 135/200\n",
            "12967/12967 [==============================] - 1s 48us/step - loss: 39736152536.8705 - mean_squared_error: 39736152536.8705\n",
            "Epoch 136/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 39618407300.4124 - mean_squared_error: 39618407300.4124\n",
            "Epoch 137/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 39519954238.1689 - mean_squared_error: 39519954238.1689\n",
            "Epoch 138/200\n",
            "12967/12967 [==============================] - 1s 48us/step - loss: 39651470621.7913 - mean_squared_error: 39651470621.7913\n",
            "Epoch 139/200\n",
            "12967/12967 [==============================] - 1s 48us/step - loss: 39470074906.4548 - mean_squared_error: 39470074906.4548\n",
            "Epoch 140/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 39451021644.6203 - mean_squared_error: 39451021644.6203\n",
            "Epoch 141/200\n",
            "12967/12967 [==============================] - 1s 59us/step - loss: 39380612228.2742 - mean_squared_error: 39380612228.2742\n",
            "Epoch 142/200\n",
            "12967/12967 [==============================] - 1s 53us/step - loss: 39389295536.7934 - mean_squared_error: 39389295536.7934\n",
            "Epoch 143/200\n",
            "12967/12967 [==============================] - 1s 48us/step - loss: 39338153807.5817 - mean_squared_error: 39338153807.5817\n",
            "Epoch 144/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 39249923810.0507 - mean_squared_error: 39249923810.0507\n",
            "Epoch 145/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 39401889821.0608 - mean_squared_error: 39401889821.0608\n",
            "Epoch 146/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 39291448523.6628 - mean_squared_error: 39291448523.6628\n",
            "Epoch 147/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 39168604054.9703 - mean_squared_error: 39168604054.9703\n",
            "Epoch 148/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 39229019718.0856 - mean_squared_error: 39229019718.0856\n",
            "Epoch 149/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 39136937023.8865 - mean_squared_error: 39136937023.8865\n",
            "Epoch 150/200\n",
            "12967/12967 [==============================] - 1s 48us/step - loss: 39079358140.6191 - mean_squared_error: 39079358140.6191\n",
            "Epoch 151/200\n",
            "12967/12967 [==============================] - 1s 47us/step - loss: 39014896212.0632 - mean_squared_error: 39014896212.0632\n",
            "Epoch 152/200\n",
            "12967/12967 [==============================] - 1s 48us/step - loss: 39049080050.6739 - mean_squared_error: 39049080050.6739\n",
            "Epoch 153/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 38917992437.4181 - mean_squared_error: 38917992437.4181\n",
            "Epoch 154/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 38991251344.6527 - mean_squared_error: 38991251344.6527\n",
            "Epoch 155/200\n",
            "12967/12967 [==============================] - 1s 48us/step - loss: 38877610795.1767 - mean_squared_error: 38877610795.1767\n",
            "Epoch 156/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 38979532188.3008 - mean_squared_error: 38979532188.3008\n",
            "Epoch 157/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 38987703446.2398 - mean_squared_error: 38987703446.2398\n",
            "Epoch 158/200\n",
            "12967/12967 [==============================] - 1s 53us/step - loss: 38961562234.0477 - mean_squared_error: 38961562234.0477\n",
            "Epoch 159/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 38954987527.5811 - mean_squared_error: 38954987527.5811\n",
            "Epoch 160/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 38970489876.8480 - mean_squared_error: 38970489876.8480\n",
            "Epoch 161/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 38867200929.7892 - mean_squared_error: 38867200929.7892\n",
            "Epoch 162/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 38795763811.5018 - mean_squared_error: 38795763811.5018\n",
            "Epoch 163/200\n",
            "12967/12967 [==============================] - 1s 58us/step - loss: 38652149340.8289 - mean_squared_error: 38652149340.8289\n",
            "Epoch 164/200\n",
            "12967/12967 [==============================] - 1s 66us/step - loss: 38729673197.5606 - mean_squared_error: 38729673197.5606\n",
            "Epoch 165/200\n",
            "12967/12967 [==============================] - 1s 63us/step - loss: 38693300587.5765 - mean_squared_error: 38693300587.5765\n",
            "Epoch 166/200\n",
            "12967/12967 [==============================] - 1s 58us/step - loss: 38856117249.3425 - mean_squared_error: 38856117249.3425\n",
            "Epoch 167/200\n",
            "12967/12967 [==============================] - 1s 64us/step - loss: 38761547050.5844 - mean_squared_error: 38761547050.5844\n",
            "Epoch 168/200\n",
            "12967/12967 [==============================] - 1s 73us/step - loss: 38653252207.2288 - mean_squared_error: 38653252207.2288\n",
            "Epoch 169/200\n",
            "12967/12967 [==============================] - 1s 55us/step - loss: 38814663585.7892 - mean_squared_error: 38814663585.7892\n",
            "Epoch 170/200\n",
            "12967/12967 [==============================] - 1s 57us/step - loss: 38716459190.5779 - mean_squared_error: 38716459190.5779\n",
            "Epoch 171/200\n",
            "12967/12967 [==============================] - 1s 57us/step - loss: 38703731064.5275 - mean_squared_error: 38703731064.5275\n",
            "Epoch 172/200\n",
            "12967/12967 [==============================] - 1s 62us/step - loss: 38457212996.2298 - mean_squared_error: 38457212996.2298\n",
            "Epoch 173/200\n",
            "12967/12967 [==============================] - 1s 63us/step - loss: 38537431369.6195 - mean_squared_error: 38537431369.6195\n",
            "Epoch 174/200\n",
            "12967/12967 [==============================] - 1s 58us/step - loss: 38541411041.1031 - mean_squared_error: 38541411041.1031\n",
            "Epoch 175/200\n",
            "12967/12967 [==============================] - 1s 58us/step - loss: 38651359978.2636 - mean_squared_error: 38651359978.2636\n",
            "Epoch 176/200\n",
            "12967/12967 [==============================] - 1s 55us/step - loss: 38419115672.0561 - mean_squared_error: 38419115672.0561\n",
            "Epoch 177/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 38453594728.1215 - mean_squared_error: 38453594728.1215\n",
            "Epoch 178/200\n",
            "12967/12967 [==============================] - 1s 47us/step - loss: 38476468608.9772 - mean_squared_error: 38476468608.9772\n",
            "Epoch 179/200\n",
            "12967/12967 [==============================] - 1s 63us/step - loss: 38484250557.8234 - mean_squared_error: 38484250557.8234\n",
            "Epoch 180/200\n",
            "12967/12967 [==============================] - 1s 65us/step - loss: 38435229264.3517 - mean_squared_error: 38435229264.3517\n",
            "Epoch 181/200\n",
            "12967/12967 [==============================] - 1s 70us/step - loss: 38347338588.7696 - mean_squared_error: 38347338588.7696\n",
            "Epoch 182/200\n",
            "12967/12967 [==============================] - 1s 62us/step - loss: 38346174152.9384 - mean_squared_error: 38346174152.9384\n",
            "Epoch 183/200\n",
            "12967/12967 [==============================] - 1s 77us/step - loss: 38215642153.3011 - mean_squared_error: 38215642153.3011\n",
            "Epoch 184/200\n",
            "12967/12967 [==============================] - 1s 66us/step - loss: 38278777086.1245 - mean_squared_error: 38278777086.1245\n",
            "Epoch 185/200\n",
            "12967/12967 [==============================] - 1s 62us/step - loss: 38195944015.7989 - mean_squared_error: 38195944015.7989\n",
            "Epoch 186/200\n",
            "12967/12967 [==============================] - 1s 65us/step - loss: 38411511707.2347 - mean_squared_error: 38411511707.2347\n",
            "Epoch 187/200\n",
            "12967/12967 [==============================] - 1s 65us/step - loss: 38279013863.4799 - mean_squared_error: 38279013863.4799\n",
            "Epoch 188/200\n",
            "12967/12967 [==============================] - 1s 57us/step - loss: 38214195971.2180 - mean_squared_error: 38214195971.2180\n",
            "Epoch 189/200\n",
            "12967/12967 [==============================] - 1s 55us/step - loss: 38173426523.1903 - mean_squared_error: 38173426523.1903\n",
            "Epoch 190/200\n",
            "12967/12967 [==============================] - 1s 55us/step - loss: 38200637208.6188 - mean_squared_error: 38200637208.6188\n",
            "Epoch 191/200\n",
            "12967/12967 [==============================] - 1s 54us/step - loss: 38213878979.2921 - mean_squared_error: 38213878979.2921\n",
            "Epoch 192/200\n",
            "12967/12967 [==============================] - 1s 48us/step - loss: 38331378974.8179 - mean_squared_error: 38331378974.8179\n",
            "Epoch 193/200\n",
            "12967/12967 [==============================] - 1s 47us/step - loss: 38146752727.4293 - mean_squared_error: 38146752727.4293\n",
            "Epoch 194/200\n",
            "12967/12967 [==============================] - 1s 47us/step - loss: 38103756575.6866 - mean_squared_error: 38103756575.6866\n",
            "Epoch 195/200\n",
            "12967/12967 [==============================] - 1s 45us/step - loss: 37911549714.7750 - mean_squared_error: 37911549714.7750\n",
            "Epoch 196/200\n",
            "12967/12967 [==============================] - 1s 46us/step - loss: 38086086178.7072 - mean_squared_error: 38086086178.7072\n",
            "Epoch 197/200\n",
            "12967/12967 [==============================] - 1s 48us/step - loss: 38059504075.9195 - mean_squared_error: 38059504075.9195\n",
            "Epoch 198/200\n",
            "12967/12967 [==============================] - 1s 48us/step - loss: 38039908484.1953 - mean_squared_error: 38039908484.1953\n",
            "Epoch 199/200\n",
            "12967/12967 [==============================] - 1s 47us/step - loss: 37965203543.9722 - mean_squared_error: 37965203543.9722\n",
            "Epoch 200/200\n",
            "12967/12967 [==============================] - 1s 47us/step - loss: 38007365433.4702 - mean_squared_error: 38007365433.4702\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1e5f59bc88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ul_SNVELmVrX",
        "colab_type": "code",
        "outputId": "e6f57634-5c05-499a-a6ec-5d39e0ebe575",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "#12967/12967 [==============================] - 1s 47us/step - loss: 38007365433.4702 - mean_squared_error: 38007365433.4702\n",
        "#<keras.callbacks.History at 0x7f1e5f59bc88>\n",
        "\n",
        "ANN3_pred = model.predict(Xva)\n",
        "score = np.sqrt(mean_squared_error(Yva,ANN3_pred))\n",
        "\n",
        "for i in np.arange(0,5):\n",
        "  print(f'yhat={pred[i]} y= {Yva.iloc[i]}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "yhat=[429908.03] y= 440000.0\n",
            "yhat=[503415.03] y= 278226.0\n",
            "yhat=[447634.2] y= 304000.0\n",
            "yhat=[396620.34] y= 265000.0\n",
            "yhat=[368484.7] y= 475000.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtToIAoFn3z4",
        "colab_type": "code",
        "outputId": "328691af-0cfd-4776-a924-2b527e073b84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "38007365433/44455414447"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8549546980000062"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hppz38wcpDKv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpae9nWrsp8P",
        "colab_type": "code",
        "outputId": "245886cd-70c2-41b6-bfa5-0eeb76caec69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Model #4 = 3 intermediate layers (100 neurons each) - Random initializer + Regularizer\n",
        "ANN4 = Sequential()\n",
        "ANN4.add(Dense(100,kernel_initializer='random_normal', input_dim=len(Xtr.columns), activation= \"relu\"))\n",
        "ANN4.add(Dense(100,kernel_initializer='random_normal',  activation= \"relu\", kernel_regularizer=regularizers.l2(0.01)))\n",
        "ANN4.add(Dense(100,kernel_initializer='random_normal',  activation= \"relu\"))\n",
        "ANN4.add(Dense(1))\n",
        "ANN4.summary() #Print model Summary\n",
        "\n",
        "# Model Compilation\n",
        "ANN4.compile(loss= \"mean_squared_error\" , optimizer=\"adam\", metrics=[\"mean_squared_error\"])\n",
        "\n",
        "# Fit Model - 200 epochs\n",
        "ANN4.fit(Xtr, Ytr, epochs=200)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_31 (Dense)             (None, 100)               1900      \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_33 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_34 (Dense)             (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 22,201\n",
            "Trainable params: 22,201\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "12967/12967 [==============================] - 1s 105us/step - loss: 294004322973.3471 - mean_squared_error: 294004322973.3471\n",
            "Epoch 2/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 165215873333.8771 - mean_squared_error: 165215873333.8771\n",
            "Epoch 3/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 130357990968.6608 - mean_squared_error: 130357990968.6608\n",
            "Epoch 4/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 101172075104.2246 - mean_squared_error: 101172075104.2246\n",
            "Epoch 5/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 94432291627.9664 - mean_squared_error: 94432291627.9664\n",
            "Epoch 6/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 89432583500.6203 - mean_squared_error: 89432583500.6203\n",
            "Epoch 7/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 84500633416.0006 - mean_squared_error: 84500633416.0006\n",
            "Epoch 8/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 78911991146.2340 - mean_squared_error: 78911991146.2340\n",
            "Epoch 9/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 73661452286.7365 - mean_squared_error: 73661452286.7365\n",
            "Epoch 10/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 68820616329.8021 - mean_squared_error: 68820616329.8021\n",
            "Epoch 11/200\n",
            "12967/12967 [==============================] - 1s 53us/step - loss: 64729603431.7070 - mean_squared_error: 64729603431.7070\n",
            "Epoch 12/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 62101422448.1567 - mean_squared_error: 62101422448.1567\n",
            "Epoch 13/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 59934627140.4865 - mean_squared_error: 59934627140.4865\n",
            "Epoch 14/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 58134886349.1435 - mean_squared_error: 58134886349.1435\n",
            "Epoch 15/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 56896613515.2236 - mean_squared_error: 56896613515.2236\n",
            "Epoch 16/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 55818985976.1425 - mean_squared_error: 55818985976.1425\n",
            "Epoch 17/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 54864148867.7412 - mean_squared_error: 54864148867.7412\n",
            "Epoch 18/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 54297433701.8314 - mean_squared_error: 54297433701.8314\n",
            "Epoch 19/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 53626029309.1768 - mean_squared_error: 53626029309.1768\n",
            "Epoch 20/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 52870811260.4167 - mean_squared_error: 52870811260.4167\n",
            "Epoch 21/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 52183619244.9041 - mean_squared_error: 52183619244.9041\n",
            "Epoch 22/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 51749191838.0973 - mean_squared_error: 51749191838.0973\n",
            "Epoch 23/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 51196300136.6151 - mean_squared_error: 51196300136.6151\n",
            "Epoch 24/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 50723942232.9001 - mean_squared_error: 50723942232.9001\n",
            "Epoch 25/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 50178411731.4019 - mean_squared_error: 50178411731.4019\n",
            "Epoch 26/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 49731155643.9084 - mean_squared_error: 49731155643.9084\n",
            "Epoch 27/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 49280860482.0384 - mean_squared_error: 49280860482.0384\n",
            "Epoch 28/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 48848628187.0817 - mean_squared_error: 48848628187.0817\n",
            "Epoch 29/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 48543406311.6971 - mean_squared_error: 48543406311.6971\n",
            "Epoch 30/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 48127146851.8769 - mean_squared_error: 48127146851.8769\n",
            "Epoch 31/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 47909744566.4002 - mean_squared_error: 47909744566.4002\n",
            "Epoch 32/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 47614679162.5610 - mean_squared_error: 47614679162.5610\n",
            "Epoch 33/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 47307012722.7035 - mean_squared_error: 47307012722.7035\n",
            "Epoch 34/200\n",
            "12967/12967 [==============================] - 1s 53us/step - loss: 47058900792.8384 - mean_squared_error: 47058900792.8384\n",
            "Epoch 35/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 46820652214.6569 - mean_squared_error: 46820652214.6569\n",
            "Epoch 36/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 46596680530.5036 - mean_squared_error: 46596680530.5036\n",
            "Epoch 37/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 46332935344.8131 - mean_squared_error: 46332935344.8131\n",
            "Epoch 38/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 46102682328.2585 - mean_squared_error: 46102682328.2585\n",
            "Epoch 39/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 45958310824.1067 - mean_squared_error: 45958310824.1067\n",
            "Epoch 40/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 45717490607.5299 - mean_squared_error: 45717490607.5299\n",
            "Epoch 41/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 45606776048.7786 - mean_squared_error: 45606776048.7786\n",
            "Epoch 42/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 45428070364.3847 - mean_squared_error: 45428070364.3847\n",
            "Epoch 43/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 45342616978.1137 - mean_squared_error: 45342616978.1137\n",
            "Epoch 44/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 45221631437.5779 - mean_squared_error: 45221631437.5779\n",
            "Epoch 45/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 45058649000.8964 - mean_squared_error: 45058649000.8964\n",
            "Epoch 46/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 44956692463.9691 - mean_squared_error: 44956692463.9691\n",
            "Epoch 47/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 44713888318.4255 - mean_squared_error: 44713888318.4255\n",
            "Epoch 48/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 44568382993.0180 - mean_squared_error: 44568382993.0180\n",
            "Epoch 49/200\n",
            "12967/12967 [==============================] - 1s 53us/step - loss: 44466802315.5394 - mean_squared_error: 44466802315.5394\n",
            "Epoch 50/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 44469417888.2887 - mean_squared_error: 44469417888.2887\n",
            "Epoch 51/200\n",
            "12967/12967 [==============================] - 1s 53us/step - loss: 44294113190.2904 - mean_squared_error: 44294113190.2904\n",
            "Epoch 52/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 44065729234.8886 - mean_squared_error: 44065729234.8886\n",
            "Epoch 53/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 44062223961.5911 - mean_squared_error: 44062223961.5911\n",
            "Epoch 54/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 43968680456.4103 - mean_squared_error: 43968680456.4103\n",
            "Epoch 55/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 43931545970.3679 - mean_squared_error: 43931545970.3679\n",
            "Epoch 56/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 43774073846.1288 - mean_squared_error: 43774073846.1288\n",
            "Epoch 57/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 43647860846.4786 - mean_squared_error: 43647860846.4786\n",
            "Epoch 58/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 43610205982.3046 - mean_squared_error: 43610205982.3046\n",
            "Epoch 59/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 43430563798.0671 - mean_squared_error: 43430563798.0671\n",
            "Epoch 60/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 43344517351.6971 - mean_squared_error: 43344517351.6971\n",
            "Epoch 61/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 43405174469.1873 - mean_squared_error: 43405174469.1873\n",
            "Epoch 62/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 43310819523.3710 - mean_squared_error: 43310819523.3710\n",
            "Epoch 63/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 43068569628.8239 - mean_squared_error: 43068569628.8239\n",
            "Epoch 64/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 43180147077.1626 - mean_squared_error: 43180147077.1626\n",
            "Epoch 65/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 42952528485.1207 - mean_squared_error: 42952528485.1207\n",
            "Epoch 66/200\n",
            "12967/12967 [==============================] - 1s 48us/step - loss: 42945912016.5590 - mean_squared_error: 42945912016.5590\n",
            "Epoch 67/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 42786088596.8184 - mean_squared_error: 42786088596.8184\n",
            "Epoch 68/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 42769064144.4800 - mean_squared_error: 42769064144.4800\n",
            "Epoch 69/200\n",
            "12967/12967 [==============================] - 1s 48us/step - loss: 42717840728.1499 - mean_squared_error: 42717840728.1499\n",
            "Epoch 70/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 42621238620.9671 - mean_squared_error: 42621238620.9671\n",
            "Epoch 71/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 42478453900.8030 - mean_squared_error: 42478453900.8030\n",
            "Epoch 72/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 42448182685.8802 - mean_squared_error: 42448182685.8802\n",
            "Epoch 73/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 42440721486.8907 - mean_squared_error: 42440721486.8907\n",
            "Epoch 74/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 42471754100.5395 - mean_squared_error: 42471754100.5395\n",
            "Epoch 75/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 42268761261.6544 - mean_squared_error: 42268761261.6544\n",
            "Epoch 76/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 42155996612.8122 - mean_squared_error: 42155996612.8122\n",
            "Epoch 77/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 42122916331.8232 - mean_squared_error: 42122916331.8232\n",
            "Epoch 78/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 42033829739.0632 - mean_squared_error: 42033829739.0632\n",
            "Epoch 79/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 42067626001.5708 - mean_squared_error: 42067626001.5708\n",
            "Epoch 80/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 41835075628.3810 - mean_squared_error: 41835075628.3810\n",
            "Epoch 81/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 41867783453.7913 - mean_squared_error: 41867783453.7913\n",
            "Epoch 82/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 41780794616.5176 - mean_squared_error: 41780794616.5176\n",
            "Epoch 83/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 41699930552.0191 - mean_squared_error: 41699930552.0191\n",
            "Epoch 84/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 41699766927.4484 - mean_squared_error: 41699766927.4484\n",
            "Epoch 85/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 41670609026.9317 - mean_squared_error: 41670609026.9317\n",
            "Epoch 86/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 41416567623.7637 - mean_squared_error: 41416567623.7637\n",
            "Epoch 87/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 41435213875.9621 - mean_squared_error: 41435213875.9621\n",
            "Epoch 88/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 41398251317.8376 - mean_squared_error: 41398251317.8376\n",
            "Epoch 89/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 41364312445.5816 - mean_squared_error: 41364312445.5816\n",
            "Epoch 90/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 41425879970.7368 - mean_squared_error: 41425879970.7368\n",
            "Epoch 91/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 41269594301.0534 - mean_squared_error: 41269594301.0534\n",
            "Epoch 92/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 41327050464.9452 - mean_squared_error: 41327050464.9452\n",
            "Epoch 93/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 41049620696.5349 - mean_squared_error: 41049620696.5349\n",
            "Epoch 94/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 41107030983.2208 - mean_squared_error: 41107030983.2208\n",
            "Epoch 95/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 40968645030.9617 - mean_squared_error: 40968645030.9617\n",
            "Epoch 96/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 41008513904.2752 - mean_squared_error: 41008513904.2752\n",
            "Epoch 97/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 40936726070.6075 - mean_squared_error: 40936726070.6075\n",
            "Epoch 98/200\n",
            "12967/12967 [==============================] - 1s 48us/step - loss: 40825918309.1404 - mean_squared_error: 40825918309.1404\n",
            "Epoch 99/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 40931938336.2591 - mean_squared_error: 40931938336.2591\n",
            "Epoch 100/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 40850103798.1683 - mean_squared_error: 40850103798.1683\n",
            "Epoch 101/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 40763254133.1318 - mean_squared_error: 40763254133.1318\n",
            "Epoch 102/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 40636814358.5064 - mean_squared_error: 40636814358.5064\n",
            "Epoch 103/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 40652987867.0027 - mean_squared_error: 40652987867.0027\n",
            "Epoch 104/200\n",
            "12967/12967 [==============================] - 1s 48us/step - loss: 40485760438.6766 - mean_squared_error: 40485760438.6766\n",
            "Epoch 105/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 40604245630.2725 - mean_squared_error: 40604245630.2725\n",
            "Epoch 106/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 40530135569.7287 - mean_squared_error: 40530135569.7287\n",
            "Epoch 107/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 40397158532.6691 - mean_squared_error: 40397158532.6691\n",
            "Epoch 108/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 40295132187.2840 - mean_squared_error: 40295132187.2840\n",
            "Epoch 109/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 40205143345.6917 - mean_squared_error: 40205143345.6917\n",
            "Epoch 110/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 40349914199.4984 - mean_squared_error: 40349914199.4984\n",
            "Epoch 111/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 40345516800.0592 - mean_squared_error: 40345516800.0592\n",
            "Epoch 112/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 40399545803.5246 - mean_squared_error: 40399545803.5246\n",
            "Epoch 113/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 40028094891.5419 - mean_squared_error: 40028094891.5419\n",
            "Epoch 114/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 40260170056.0401 - mean_squared_error: 40260170056.0401\n",
            "Epoch 115/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 40264595475.4265 - mean_squared_error: 40264595475.4265\n",
            "Epoch 116/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 40189752565.5168 - mean_squared_error: 40189752565.5168\n",
            "Epoch 117/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 39975469292.5142 - mean_squared_error: 39975469292.5142\n",
            "Epoch 118/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 40044903564.1317 - mean_squared_error: 40044903564.1317\n",
            "Epoch 119/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 39944660324.3902 - mean_squared_error: 39944660324.3902\n",
            "Epoch 120/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 39860752759.7378 - mean_squared_error: 39860752759.7378\n",
            "Epoch 121/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 39870642454.4471 - mean_squared_error: 39870642454.4471\n",
            "Epoch 122/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 39850905310.3392 - mean_squared_error: 39850905310.3392\n",
            "Epoch 123/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 39772067524.8320 - mean_squared_error: 39772067524.8320\n",
            "Epoch 124/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 39747356418.9811 - mean_squared_error: 39747356418.9811\n",
            "Epoch 125/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 39788183977.3308 - mean_squared_error: 39788183977.3308\n",
            "Epoch 126/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 39674965114.7979 - mean_squared_error: 39674965114.7979\n",
            "Epoch 127/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 39681937097.8070 - mean_squared_error: 39681937097.8070\n",
            "Epoch 128/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 39644338719.1535 - mean_squared_error: 39644338719.1535\n",
            "Epoch 129/200\n",
            "12967/12967 [==============================] - 1s 53us/step - loss: 39623533080.2042 - mean_squared_error: 39623533080.2042\n",
            "Epoch 130/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 39426778106.3932 - mean_squared_error: 39426778106.3932\n",
            "Epoch 131/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 39633358319.5348 - mean_squared_error: 39633358319.5348\n",
            "Epoch 132/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 39517600659.2587 - mean_squared_error: 39517600659.2587\n",
            "Epoch 133/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 39568837420.3612 - mean_squared_error: 39568837420.3612\n",
            "Epoch 134/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 39546979848.4892 - mean_squared_error: 39546979848.4892\n",
            "Epoch 135/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 39430693129.8120 - mean_squared_error: 39430693129.8120\n",
            "Epoch 136/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 39438237728.6935 - mean_squared_error: 39438237728.6935\n",
            "Epoch 137/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 39389239469.5754 - mean_squared_error: 39389239469.5754\n",
            "Epoch 138/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 39339331897.6676 - mean_squared_error: 39339331897.6676\n",
            "Epoch 139/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 39263643391.6644 - mean_squared_error: 39263643391.6644\n",
            "Epoch 140/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 39299085101.7432 - mean_squared_error: 39299085101.7432\n",
            "Epoch 141/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 39182236535.7773 - mean_squared_error: 39182236535.7773\n",
            "Epoch 142/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 39085567286.6273 - mean_squared_error: 39085567286.6273\n",
            "Epoch 143/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 39114222410.2907 - mean_squared_error: 39114222410.2907\n",
            "Epoch 144/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 39121528441.9687 - mean_squared_error: 39121528441.9687\n",
            "Epoch 145/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 39059990013.2755 - mean_squared_error: 39059990013.2755\n",
            "Epoch 146/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 39213440715.2285 - mean_squared_error: 39213440715.2285\n",
            "Epoch 147/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 38992636535.2837 - mean_squared_error: 38992636535.2837\n",
            "Epoch 148/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 39125308483.9929 - mean_squared_error: 39125308483.9929\n",
            "Epoch 149/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 39041534570.0168 - mean_squared_error: 39041534570.0168\n",
            "Epoch 150/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 39084950212.1212 - mean_squared_error: 39084950212.1212\n",
            "Epoch 151/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 38954723308.5735 - mean_squared_error: 38954723308.5735\n",
            "Epoch 152/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 38998003623.7119 - mean_squared_error: 38998003623.7119\n",
            "Epoch 153/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 38796583533.0177 - mean_squared_error: 38796583533.0177\n",
            "Epoch 154/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 38970775817.8120 - mean_squared_error: 38970775817.8120\n",
            "Epoch 155/200\n",
            "12967/12967 [==============================] - 1s 53us/step - loss: 38830566775.7378 - mean_squared_error: 38830566775.7378\n",
            "Epoch 156/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 38725856325.0985 - mean_squared_error: 38725856325.0985\n",
            "Epoch 157/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 38690374919.0480 - mean_squared_error: 38690374919.0480\n",
            "Epoch 158/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 38787862526.4206 - mean_squared_error: 38787862526.4206\n",
            "Epoch 159/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 38743777780.9837 - mean_squared_error: 38743777780.9837\n",
            "Epoch 160/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 38715356625.8422 - mean_squared_error: 38715356625.8422\n",
            "Epoch 161/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 38673593333.9708 - mean_squared_error: 38673593333.9708\n",
            "Epoch 162/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 38513525165.5951 - mean_squared_error: 38513525165.5951\n",
            "Epoch 163/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 38576743547.8245 - mean_squared_error: 38576743547.8245\n",
            "Epoch 164/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 38747018743.0369 - mean_squared_error: 38747018743.0369\n",
            "Epoch 165/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 38538675460.0472 - mean_squared_error: 38538675460.0472\n",
            "Epoch 166/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 38568141264.0259 - mean_squared_error: 38568141264.0259\n",
            "Epoch 167/200\n",
            "12967/12967 [==============================] - 1s 48us/step - loss: 38452278137.0408 - mean_squared_error: 38452278137.0408\n",
            "Epoch 168/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 38515649042.2025 - mean_squared_error: 38515649042.2025\n",
            "Epoch 169/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 38409669455.3448 - mean_squared_error: 38409669455.3448\n",
            "Epoch 170/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 38526279839.2029 - mean_squared_error: 38526279839.2029\n",
            "Epoch 171/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 38356912386.5468 - mean_squared_error: 38356912386.5468\n",
            "Epoch 172/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 38398379735.2319 - mean_squared_error: 38398379735.2319\n",
            "Epoch 173/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 38379723624.2992 - mean_squared_error: 38379723624.2992\n",
            "Epoch 174/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 38393312234.5992 - mean_squared_error: 38393312234.5992\n",
            "Epoch 175/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 38311925773.8987 - mean_squared_error: 38311925773.8987\n",
            "Epoch 176/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 38338093594.2574 - mean_squared_error: 38338093594.2574\n",
            "Epoch 177/200\n",
            "12967/12967 [==============================] - 1s 53us/step - loss: 38205306557.0140 - mean_squared_error: 38205306557.0140\n",
            "Epoch 178/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 38388935268.3310 - mean_squared_error: 38388935268.3310\n",
            "Epoch 179/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 38272727518.4774 - mean_squared_error: 38272727518.4774\n",
            "Epoch 180/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 38154718219.2137 - mean_squared_error: 38154718219.2137\n",
            "Epoch 181/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 38182621590.6939 - mean_squared_error: 38182621590.6939\n",
            "Epoch 182/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 38338981360.9958 - mean_squared_error: 38338981360.8576\n",
            "Epoch 183/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 38311025753.2357 - mean_squared_error: 38311025753.2357\n",
            "Epoch 184/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 38109022286.9697 - mean_squared_error: 38109022286.9697\n",
            "Epoch 185/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 38123847831.5428 - mean_squared_error: 38123847831.5428\n",
            "Epoch 186/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 38135320520.4843 - mean_squared_error: 38135320520.4843\n",
            "Epoch 187/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 37945044017.0797 - mean_squared_error: 37945044017.0797\n",
            "Epoch 188/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 38006545298.0742 - mean_squared_error: 38006545298.0742\n",
            "Epoch 189/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 38062866403.7289 - mean_squared_error: 38062866403.7289\n",
            "Epoch 190/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 38007968677.4218 - mean_squared_error: 38007968677.4218\n",
            "Epoch 191/200\n",
            "12967/12967 [==============================] - 1s 48us/step - loss: 38130733039.5743 - mean_squared_error: 38130733039.5743\n",
            "Epoch 192/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 37897102127.4410 - mean_squared_error: 37897102127.4410\n",
            "Epoch 193/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 37966792173.7975 - mean_squared_error: 37966792173.7975\n",
            "Epoch 194/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 37943031530.0267 - mean_squared_error: 37943031530.0267\n",
            "Epoch 195/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 37897190984.9285 - mean_squared_error: 37897190984.9285\n",
            "Epoch 196/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 38057080391.1122 - mean_squared_error: 38057080391.1122\n",
            "Epoch 197/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 37953897794.4333 - mean_squared_error: 37953897794.4333\n",
            "Epoch 198/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 37948783178.5869 - mean_squared_error: 37948783178.5869\n",
            "Epoch 199/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 37827303106.2260 - mean_squared_error: 37827303106.2260\n",
            "Epoch 200/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 37881292217.0457 - mean_squared_error: 37881292217.0457\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1e5f0b6668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysnM3rURt38j",
        "colab_type": "code",
        "outputId": "31feb6b5-b577-4f6d-b761-e155946eee03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "#12967/12967 [==============================] - 1s 50us/step - loss: 37881292217.0457 - mean_squared_error: 37881292217.0457\n",
        "#<keras.callbacks.History at 0x7f1e5f0b6668>\n",
        "\n",
        "ANN4_pred = model.predict(Xva)\n",
        "score = np.sqrt(mean_squared_error(Yva,ANN4_pred))\n",
        "\n",
        "for i in np.arange(0,5):\n",
        "  print(f'yhat={pred[i]} y= {Yva.iloc[i]}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "yhat=[429908.03] y= 440000.0\n",
            "yhat=[503415.03] y= 278226.0\n",
            "yhat=[447634.2] y= 304000.0\n",
            "yhat=[396620.34] y= 265000.0\n",
            "yhat=[368484.7] y= 475000.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soZFfaLEvQYU",
        "colab_type": "code",
        "outputId": "82250a07-603e-43ca-cfc4-e634cdea357e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "37881292217/38007365433"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9966829267284457"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpCeAgJ_vVWL",
        "colab_type": "code",
        "outputId": "c786fe60-c249-47a6-8e16-58e2a9c01d48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Model #5 = 3 intermediate layers (100 neurons each) - Random initializer + Regularizer + Optimizer = adadelta\n",
        "\n",
        "ANN5 = Sequential()\n",
        "ANN5.add(Dense(100,kernel_initializer='random_normal', input_dim=len(Xtr.columns), activation= \"relu\"))\n",
        "ANN5.add(Dense(100,kernel_initializer='random_normal',  activation= \"relu\"))\n",
        "ANN5.add(Dense(100,kernel_initializer='random_normal',  activation= \"relu\"))\n",
        "ANN5.add(Dense(1))\n",
        "ANN5.summary() #Print model Summary\n",
        "\n",
        "# Model Compilation\n",
        "\n",
        "ANN5.compile(loss= \"mean_squared_error\" , optimizer='adadelta', metrics=[\"mean_squared_error\"])\n",
        "\n",
        "# Fit Model - 200 epochs\n",
        "ANN5.fit(Xtr, Ytr, epochs=200)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_59 (Dense)             (None, 100)               1900      \n",
            "_________________________________________________________________\n",
            "dense_60 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_61 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_62 (Dense)             (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 22,201\n",
            "Trainable params: 22,201\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "12967/12967 [==============================] - 2s 131us/step - loss: 108271661193.0914 - mean_squared_error: 108271661193.0914\n",
            "Epoch 2/200\n",
            "12967/12967 [==============================] - 1s 54us/step - loss: 53843390659.6079 - mean_squared_error: 53843390659.6079\n",
            "Epoch 3/200\n",
            "12967/12967 [==============================] - 1s 54us/step - loss: 51053545209.0309 - mean_squared_error: 51053545209.0309\n",
            "Epoch 4/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 50237031650.8799 - mean_squared_error: 50237031650.8799\n",
            "Epoch 5/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 49801688840.1931 - mean_squared_error: 49801688840.1931\n",
            "Epoch 6/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 49521846935.3454 - mean_squared_error: 49521846935.3454\n",
            "Epoch 7/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 49232591134.6600 - mean_squared_error: 49232591134.6600\n",
            "Epoch 8/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 48943585332.9097 - mean_squared_error: 48943585332.9097\n",
            "Epoch 9/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 48730778191.0092 - mean_squared_error: 48730778191.0092\n",
            "Epoch 10/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 48530102653.8185 - mean_squared_error: 48530102653.8185\n",
            "Epoch 11/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 48326234786.8750 - mean_squared_error: 48326234786.8750\n",
            "Epoch 12/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 48091898815.0079 - mean_squared_error: 48091898815.0079\n",
            "Epoch 13/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 47897918066.1507 - mean_squared_error: 47897918066.1507\n",
            "Epoch 14/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 47704868250.6424 - mean_squared_error: 47704868250.6424\n",
            "Epoch 15/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 47460383011.3982 - mean_squared_error: 47460383011.3982\n",
            "Epoch 16/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 47262645525.2626 - mean_squared_error: 47262645525.2626\n",
            "Epoch 17/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 46979046969.1346 - mean_squared_error: 46979046969.1346\n",
            "Epoch 18/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 46919687352.3942 - mean_squared_error: 46919687352.3942\n",
            "Epoch 19/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 46573534880.4269 - mean_squared_error: 46573534880.4269\n",
            "Epoch 20/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 46477527540.3520 - mean_squared_error: 46477527540.3520\n",
            "Epoch 21/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 46254611892.3075 - mean_squared_error: 46254611892.3075\n",
            "Epoch 22/200\n",
            "12967/12967 [==============================] - 1s 53us/step - loss: 46002343008.0272 - mean_squared_error: 46002343008.0272\n",
            "Epoch 23/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 45939605154.4801 - mean_squared_error: 45939605154.4801\n",
            "Epoch 24/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 45662345729.4609 - mean_squared_error: 45662345729.4609\n",
            "Epoch 25/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 45517818054.8457 - mean_squared_error: 45517818054.8457\n",
            "Epoch 26/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 45258356526.3355 - mean_squared_error: 45258356526.3355\n",
            "Epoch 27/200\n",
            "12967/12967 [==============================] - 1s 53us/step - loss: 45122678601.5800 - mean_squared_error: 45122678601.5800\n",
            "Epoch 28/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 44959697306.1291 - mean_squared_error: 44959697306.1291\n",
            "Epoch 29/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 44818420218.0378 - mean_squared_error: 44818420218.0378\n",
            "Epoch 30/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 44678616293.5649 - mean_squared_error: 44678616293.5649\n",
            "Epoch 31/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 44519928313.0901 - mean_squared_error: 44519928313.0901\n",
            "Epoch 32/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 44332175012.1385 - mean_squared_error: 44332175012.1385\n",
            "Epoch 33/200\n",
            "12967/12967 [==============================] - 1s 53us/step - loss: 44223845986.5147 - mean_squared_error: 44223845986.5147\n",
            "Epoch 34/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 44151700027.8195 - mean_squared_error: 44151700027.8195\n",
            "Epoch 35/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 43961550646.9432 - mean_squared_error: 43961550646.9432\n",
            "Epoch 36/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 43803845603.3340 - mean_squared_error: 43803845603.3340\n",
            "Epoch 37/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 43661585991.3491 - mean_squared_error: 43661585991.3491\n",
            "Epoch 38/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 43658957577.3776 - mean_squared_error: 43658957577.3776\n",
            "Epoch 39/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 43448716286.8944 - mean_squared_error: 43448716286.8944\n",
            "Epoch 40/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 43395962706.5826 - mean_squared_error: 43395962706.5826\n",
            "Epoch 41/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 43292589237.7093 - mean_squared_error: 43292589237.7093\n",
            "Epoch 42/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 43166267444.5148 - mean_squared_error: 43166267444.5148\n",
            "Epoch 43/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 43123181446.0708 - mean_squared_error: 43123181446.0708\n",
            "Epoch 44/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 42931743895.8587 - mean_squared_error: 42931743895.8587\n",
            "Epoch 45/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 42888172747.3470 - mean_squared_error: 42888172747.3470\n",
            "Epoch 46/200\n",
            "12967/12967 [==============================] - 1s 54us/step - loss: 42760639048.2573 - mean_squared_error: 42760639048.2573\n",
            "Epoch 47/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 42544358301.4458 - mean_squared_error: 42544358301.4458\n",
            "Epoch 48/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 42541564404.6678 - mean_squared_error: 42541564404.6678\n",
            "Epoch 49/200\n",
            "12967/12967 [==============================] - 1s 54us/step - loss: 42425677128.2770 - mean_squared_error: 42425677128.2770\n",
            "Epoch 50/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 42335053022.7735 - mean_squared_error: 42335053022.7735\n",
            "Epoch 51/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 42264851048.5164 - mean_squared_error: 42264851048.5164\n",
            "Epoch 52/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 42078968671.2967 - mean_squared_error: 42078968671.2967\n",
            "Epoch 53/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 42040076012.7117 - mean_squared_error: 42040076012.7117\n",
            "Epoch 54/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 41903529784.6015 - mean_squared_error: 41903529784.6015\n",
            "Epoch 55/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 41870789473.0340 - mean_squared_error: 41870789473.0340\n",
            "Epoch 56/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 41730779081.8268 - mean_squared_error: 41730779081.8268\n",
            "Epoch 57/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 41666507752.0722 - mean_squared_error: 41666507752.0722\n",
            "Epoch 58/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 41558070474.8337 - mean_squared_error: 41558070474.8337\n",
            "Epoch 59/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 41504138397.4656 - mean_squared_error: 41504138397.4656\n",
            "Epoch 60/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 41439862695.1591 - mean_squared_error: 41439862695.1591\n",
            "Epoch 61/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 41328758598.7371 - mean_squared_error: 41328758598.7371\n",
            "Epoch 62/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 41206290607.1548 - mean_squared_error: 41206290607.1548\n",
            "Epoch 63/200\n",
            "12967/12967 [==============================] - 1s 54us/step - loss: 41045754556.6191 - mean_squared_error: 41045754556.6191\n",
            "Epoch 64/200\n",
            "12967/12967 [==============================] - 1s 58us/step - loss: 41059661689.9095 - mean_squared_error: 41059661689.9095\n",
            "Epoch 65/200\n",
            "12967/12967 [==============================] - 1s 56us/step - loss: 40974426694.6384 - mean_squared_error: 40974426694.6384\n",
            "Epoch 66/200\n",
            "12967/12967 [==============================] - 1s 56us/step - loss: 40871731612.8536 - mean_squared_error: 40871731612.8536\n",
            "Epoch 67/200\n",
            "12967/12967 [==============================] - 1s 56us/step - loss: 40849494822.5964 - mean_squared_error: 40849494822.5964\n",
            "Epoch 68/200\n",
            "12967/12967 [==============================] - 1s 58us/step - loss: 40783004960.4763 - mean_squared_error: 40783004960.4763\n",
            "Epoch 69/200\n",
            "12967/12967 [==============================] - 1s 62us/step - loss: 40685383731.6462 - mean_squared_error: 40685383731.6462\n",
            "Epoch 70/200\n",
            "12967/12967 [==============================] - 1s 61us/step - loss: 40588200354.3814 - mean_squared_error: 40588200354.3814\n",
            "Epoch 71/200\n",
            "12967/12967 [==============================] - 1s 65us/step - loss: 40487931752.2203 - mean_squared_error: 40487931752.2203\n",
            "Epoch 72/200\n",
            "12967/12967 [==============================] - 1s 66us/step - loss: 40426209644.0503 - mean_squared_error: 40426209644.0503\n",
            "Epoch 73/200\n",
            "12967/12967 [==============================] - 1s 64us/step - loss: 40315510043.5802 - mean_squared_error: 40315510043.5802\n",
            "Epoch 74/200\n",
            "12967/12967 [==============================] - 1s 60us/step - loss: 40276008504.5028 - mean_squared_error: 40276008504.5028\n",
            "Epoch 75/200\n",
            "12967/12967 [==============================] - 1s 61us/step - loss: 40243564221.1719 - mean_squared_error: 40243564221.1719\n",
            "Epoch 76/200\n",
            "12967/12967 [==============================] - 1s 61us/step - loss: 40064991143.2381 - mean_squared_error: 40064991143.2381\n",
            "Epoch 77/200\n",
            "12967/12967 [==============================] - 1s 56us/step - loss: 40054700431.5077 - mean_squared_error: 40054700431.5077\n",
            "Epoch 78/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 40060325806.5822 - mean_squared_error: 40060325806.5822\n",
            "Epoch 79/200\n",
            "12967/12967 [==============================] - 1s 57us/step - loss: 39903788711.3763 - mean_squared_error: 39903788711.3763\n",
            "Epoch 80/200\n",
            "12967/12967 [==============================] - 1s 61us/step - loss: 39885688471.3454 - mean_squared_error: 39885688471.3454\n",
            "Epoch 81/200\n",
            "12967/12967 [==============================] - 1s 56us/step - loss: 39767386548.8603 - mean_squared_error: 39767386548.8603\n",
            "Epoch 82/200\n",
            "12967/12967 [==============================] - 1s 57us/step - loss: 39723950595.5142 - mean_squared_error: 39723950595.5142\n",
            "Epoch 83/200\n",
            "12967/12967 [==============================] - 1s 63us/step - loss: 39619853868.1046 - mean_squared_error: 39619853868.1046\n",
            "Epoch 84/200\n",
            "12967/12967 [==============================] - 1s 58us/step - loss: 39608597726.6156 - mean_squared_error: 39608597726.6156\n",
            "Epoch 85/200\n",
            "12967/12967 [==============================] - 1s 58us/step - loss: 39490943042.8084 - mean_squared_error: 39490943042.8084\n",
            "Epoch 86/200\n",
            "12967/12967 [==============================] - 1s 59us/step - loss: 39434066640.7564 - mean_squared_error: 39434066640.7564\n",
            "Epoch 87/200\n",
            "12967/12967 [==============================] - 1s 58us/step - loss: 39322233121.1870 - mean_squared_error: 39322233121.1870\n",
            "Epoch 88/200\n",
            "12967/12967 [==============================] - 1s 66us/step - loss: 39313108866.9120 - mean_squared_error: 39313108866.9120\n",
            "Epoch 89/200\n",
            "12967/12967 [==============================] - 1s 62us/step - loss: 39151905205.6500 - mean_squared_error: 39151905205.6500\n",
            "Epoch 90/200\n",
            "12967/12967 [==============================] - 1s 62us/step - loss: 39155129983.8914 - mean_squared_error: 39155129983.8914\n",
            "Epoch 91/200\n",
            "12967/12967 [==============================] - 1s 62us/step - loss: 39127821811.5623 - mean_squared_error: 39127821811.5623\n",
            "Epoch 92/200\n",
            "12967/12967 [==============================] - 1s 59us/step - loss: 39070021199.0882 - mean_squared_error: 39070021199.0882\n",
            "Epoch 93/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 38961411397.4341 - mean_squared_error: 38961411397.4341\n",
            "Epoch 94/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 38954044366.2491 - mean_squared_error: 38954044366.2491\n",
            "Epoch 95/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 38915059698.1013 - mean_squared_error: 38915059698.1013\n",
            "Epoch 96/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 38781796166.6976 - mean_squared_error: 38781796166.6976\n",
            "Epoch 97/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 38736377696.4812 - mean_squared_error: 38736377696.4812\n",
            "Epoch 98/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 38686889562.8546 - mean_squared_error: 38686889562.8546\n",
            "Epoch 99/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 38671658495.9605 - mean_squared_error: 38671658495.9605\n",
            "Epoch 100/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 38556229178.4771 - mean_squared_error: 38556229178.4771\n",
            "Epoch 101/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 38551475497.3999 - mean_squared_error: 38551475497.3999\n",
            "Epoch 102/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 38505187424.3430 - mean_squared_error: 38505187424.3430\n",
            "Epoch 103/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 38408310790.0017 - mean_squared_error: 38408310790.0017\n",
            "Epoch 104/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 38292280587.3124 - mean_squared_error: 38292280587.3124\n",
            "Epoch 105/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 38270401135.1498 - mean_squared_error: 38270401135.1498\n",
            "Epoch 106/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 38254126410.4882 - mean_squared_error: 38254126410.4882\n",
            "Epoch 107/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 38171662492.5179 - mean_squared_error: 38171662492.5179\n",
            "Epoch 108/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 38118978315.5888 - mean_squared_error: 38118978315.5888\n",
            "Epoch 109/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 38056708840.2104 - mean_squared_error: 38056708840.2104\n",
            "Epoch 110/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 38023347156.6062 - mean_squared_error: 38023347156.6062\n",
            "Epoch 111/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 37925278066.9996 - mean_squared_error: 37925278066.9996\n",
            "Epoch 112/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 37963144825.1000 - mean_squared_error: 37963144825.1000\n",
            "Epoch 113/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 37869321815.1431 - mean_squared_error: 37869321815.1431\n",
            "Epoch 114/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 37725857340.9251 - mean_squared_error: 37725857340.9251\n",
            "Epoch 115/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 37664957174.1090 - mean_squared_error: 37664957174.1090\n",
            "Epoch 116/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 37699772215.1801 - mean_squared_error: 37699772215.1801\n",
            "Epoch 117/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 37581936914.0248 - mean_squared_error: 37581936914.0248\n",
            "Epoch 118/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 37619032224.9402 - mean_squared_error: 37619032224.9402\n",
            "Epoch 119/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 37463129278.0800 - mean_squared_error: 37463129278.0800\n",
            "Epoch 120/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 37439572754.6961 - mean_squared_error: 37439572754.6961\n",
            "Epoch 121/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 37423978463.3065 - mean_squared_error: 37423978463.3065\n",
            "Epoch 122/200\n",
            "12967/12967 [==============================] - 1s 53us/step - loss: 37358908320.6836 - mean_squared_error: 37358908320.6836\n",
            "Epoch 123/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 37235559552.4047 - mean_squared_error: 37235559552.4047\n",
            "Epoch 124/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 37246110214.3571 - mean_squared_error: 37246110214.3571\n",
            "Epoch 125/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 37148652723.1822 - mean_squared_error: 37148652723.1822\n",
            "Epoch 126/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 37179626050.3740 - mean_squared_error: 37179626050.3740\n",
            "Epoch 127/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 37004148240.3072 - mean_squared_error: 37004148240.3072\n",
            "Epoch 128/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 37011483604.0139 - mean_squared_error: 37011483604.0139\n",
            "Epoch 129/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 36978309155.2205 - mean_squared_error: 36978309155.2205\n",
            "Epoch 130/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 36934925461.9634 - mean_squared_error: 36934925461.9634\n",
            "Epoch 131/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 36883561512.1956 - mean_squared_error: 36883561512.1956\n",
            "Epoch 132/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 36824567918.8735 - mean_squared_error: 36824567918.8735\n",
            "Epoch 133/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 36825088943.6878 - mean_squared_error: 36825088943.6878\n",
            "Epoch 134/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 36689139267.4006 - mean_squared_error: 36689139267.4006\n",
            "Epoch 135/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 36722301400.7126 - mean_squared_error: 36722301400.7126\n",
            "Epoch 136/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 36568181210.1340 - mean_squared_error: 36568181210.1340\n",
            "Epoch 137/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 36531911925.6747 - mean_squared_error: 36531911925.6747\n",
            "Epoch 138/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 36555708548.5111 - mean_squared_error: 36555708548.5111\n",
            "Epoch 139/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 36451582923.9590 - mean_squared_error: 36451582923.9590\n",
            "Epoch 140/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 36382191324.5229 - mean_squared_error: 36382191324.5229\n",
            "Epoch 141/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 36413733288.9359 - mean_squared_error: 36413733288.9359\n",
            "Epoch 142/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 36291772432.1098 - mean_squared_error: 36291772432.1098\n",
            "Epoch 143/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 36222704703.8865 - mean_squared_error: 36222704703.8865\n",
            "Epoch 144/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 36217247611.6468 - mean_squared_error: 36217247611.6468\n",
            "Epoch 145/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 36166630335.3238 - mean_squared_error: 36166630335.3238\n",
            "Epoch 146/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 36110222834.6146 - mean_squared_error: 36110222834.6146\n",
            "Epoch 147/200\n",
            "12967/12967 [==============================] - 1s 53us/step - loss: 36042762361.4554 - mean_squared_error: 36042762361.4554\n",
            "Epoch 148/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 35983267684.8246 - mean_squared_error: 35983267684.8246\n",
            "Epoch 149/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 35932578344.5509 - mean_squared_error: 35932578344.5509\n",
            "Epoch 150/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 35920759758.8809 - mean_squared_error: 35920759758.8809\n",
            "Epoch 151/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 35773785895.4651 - mean_squared_error: 35773785895.4651\n",
            "Epoch 152/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 35757996427.6382 - mean_squared_error: 35757996427.6382\n",
            "Epoch 153/200\n",
            "12967/12967 [==============================] - 1s 53us/step - loss: 35699501126.6779 - mean_squared_error: 35699501126.6779\n",
            "Epoch 154/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 35687977780.2582 - mean_squared_error: 35687977780.2582\n",
            "Epoch 155/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 35560461101.5458 - mean_squared_error: 35560461101.5458\n",
            "Epoch 156/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 35540373631.4571 - mean_squared_error: 35540373631.4571\n",
            "Epoch 157/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 35496106031.8556 - mean_squared_error: 35496106031.8556\n",
            "Epoch 158/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 35446875448.2462 - mean_squared_error: 35446875448.2462\n",
            "Epoch 159/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 35319957322.0538 - mean_squared_error: 35319957322.0538\n",
            "Epoch 160/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 35280015986.3876 - mean_squared_error: 35280015986.3876\n",
            "Epoch 161/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 35305709630.5440 - mean_squared_error: 35305709630.5440\n",
            "Epoch 162/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 35202260196.6962 - mean_squared_error: 35202260196.6962\n",
            "Epoch 163/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 35107776947.3994 - mean_squared_error: 35107776947.3994\n",
            "Epoch 164/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 35033779210.7399 - mean_squared_error: 35033779210.7399\n",
            "Epoch 165/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 34919169486.0912 - mean_squared_error: 34919169486.0912\n",
            "Epoch 166/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 34825207008.2739 - mean_squared_error: 34825207008.2739\n",
            "Epoch 167/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 34844577373.7765 - mean_squared_error: 34844577373.7765\n",
            "Epoch 168/200\n",
            "12967/12967 [==============================] - 1s 53us/step - loss: 34667825358.1109 - mean_squared_error: 34667825358.1109\n",
            "Epoch 169/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 34532774742.4521 - mean_squared_error: 34532774742.4521\n",
            "Epoch 170/200\n",
            "12967/12967 [==============================] - 1s 53us/step - loss: 34487414597.3156 - mean_squared_error: 34487414597.3156\n",
            "Epoch 171/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 34410124894.8821 - mean_squared_error: 34410124894.8821\n",
            "Epoch 172/200\n",
            "12967/12967 [==============================] - 1s 54us/step - loss: 34350152036.4692 - mean_squared_error: 34350152036.4692\n",
            "Epoch 173/200\n",
            "12967/12967 [==============================] - 1s 54us/step - loss: 34288861330.8836 - mean_squared_error: 34288861330.8836\n",
            "Epoch 174/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 34220387648.1431 - mean_squared_error: 34220387648.1431\n",
            "Epoch 175/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 34081387745.1426 - mean_squared_error: 34081387745.1426\n",
            "Epoch 176/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 34077706324.2607 - mean_squared_error: 34077706324.2607\n",
            "Epoch 177/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 33978356174.9993 - mean_squared_error: 33978356174.9993\n",
            "Epoch 178/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 33855534830.9228 - mean_squared_error: 33855534830.9228\n",
            "Epoch 179/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 33833867461.1873 - mean_squared_error: 33833867461.1873\n",
            "Epoch 180/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 33761254722.9071 - mean_squared_error: 33761254722.9071\n",
            "Epoch 181/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 33620837822.9685 - mean_squared_error: 33620837822.9685\n",
            "Epoch 182/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 33550311535.6631 - mean_squared_error: 33550311535.6631\n",
            "Epoch 183/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 33516225810.8935 - mean_squared_error: 33516225810.8935\n",
            "Epoch 184/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 33358211153.4967 - mean_squared_error: 33358211153.4967\n",
            "Epoch 185/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 33336753609.7873 - mean_squared_error: 33336753609.7873\n",
            "Epoch 186/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 33271582444.0799 - mean_squared_error: 33271582444.0799\n",
            "Epoch 187/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 33181961232.1888 - mean_squared_error: 33181961232.1888\n",
            "Epoch 188/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 33172062149.4045 - mean_squared_error: 33172062149.4045\n",
            "Epoch 189/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 33084435895.5453 - mean_squared_error: 33084435895.5453\n",
            "Epoch 190/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 32988739870.7390 - mean_squared_error: 32988739870.7390\n",
            "Epoch 191/200\n",
            "12967/12967 [==============================] - 1s 51us/step - loss: 32918705028.9257 - mean_squared_error: 32918705028.9257\n",
            "Epoch 192/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 32891818160.5762 - mean_squared_error: 32891818160.5762\n",
            "Epoch 193/200\n",
            "12967/12967 [==============================] - 1s 52us/step - loss: 32821092681.7774 - mean_squared_error: 32821092681.7774\n",
            "Epoch 194/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 32739695361.0069 - mean_squared_error: 32739695361.0069\n",
            "Epoch 195/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 32652933604.8739 - mean_squared_error: 32652933604.8739\n",
            "Epoch 196/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 32586907588.9307 - mean_squared_error: 32586907588.9307\n",
            "Epoch 197/200\n",
            "12967/12967 [==============================] - 1s 50us/step - loss: 32488719313.0525 - mean_squared_error: 32488719313.0525\n",
            "Epoch 198/200\n",
            "12967/12967 [==============================] - 1s 49us/step - loss: 32403479306.9570 - mean_squared_error: 32403479306.9570\n",
            "Epoch 199/200\n",
            "12967/12967 [==============================] - 1s 53us/step - loss: 32358383475.2760 - mean_squared_error: 32358383475.2760\n",
            "Epoch 200/200\n",
            "12967/12967 [==============================] - 1s 53us/step - loss: 32357428027.1286 - mean_squared_error: 32357428027.1286\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1e5dce6eb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnTgtIrRwQro",
        "colab_type": "code",
        "outputId": "e7ebcd4f-4870-49b1-82b8-3093d4b52a65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "32357428027.1286/44003656324.0768"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.735334986457116"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5kQUitDxb17",
        "colab_type": "code",
        "outputId": "19166a99-17a4-49ed-d1ca-dfb4df7229f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        }
      },
      "source": [
        "#12967/12967 [==============================] - 1s 53us/step - loss: 32357428027.1286 - mean_squared_error: 32357428027.1286\n",
        "#<keras.callbacks.History at 0x7f1e5dce6eb8>\n",
        "\n",
        "ANN5_pred = model.predict(Xts)\n",
        "score = np.sqrt(mean_squared_error(Yts,ANN5_pred))\n",
        "\n",
        "for i in np.arange(1000,1025):\n",
        "  print(f'yhat={pred[i]} y= {Yts.iloc[i]}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "yhat=[420094.75] y= 550000.0\n",
            "yhat=[662372.3] y= 700000.0\n",
            "yhat=[387545.38] y= 645000.0\n",
            "yhat=[570496.25] y= 321000.0\n",
            "yhat=[317100.34] y= 258500.0\n",
            "yhat=[294054.25] y= 297000.0\n",
            "yhat=[295295.5] y= 488000.0\n",
            "yhat=[783523.4] y= 455000.0\n",
            "yhat=[410533.97] y= 1330000.0\n",
            "yhat=[552203.] y= 279000.0\n",
            "yhat=[1003704.94] y= 329950.0\n",
            "yhat=[484534.5] y= 610000.0\n",
            "yhat=[410272.3] y= 315000.0\n",
            "yhat=[502391.72] y= 426000.0\n",
            "yhat=[364142.47] y= 550000.0\n",
            "yhat=[325498.25] y= 383001.0\n",
            "yhat=[573375.3] y= 600000.0\n",
            "yhat=[957534.4] y= 450000.0\n",
            "yhat=[442826.03] y= 525000.0\n",
            "yhat=[516934.06] y= 575000.0\n",
            "yhat=[355372.62] y= 349000.0\n",
            "yhat=[803591.75] y= 269900.0\n",
            "yhat=[324255.03] y= 1225000.0\n",
            "yhat=[350986.44] y= 160000.0\n",
            "yhat=[396301.22] y= 525000.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5l-tej1zVUj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}